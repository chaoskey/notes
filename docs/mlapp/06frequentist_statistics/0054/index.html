<!DOCTYPE html>
<html lang="cn">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="6.2 估计器的采样分布"><meta property="og:title" content="6.2 估计器的采样分布" />
<meta property="og:description" content="返回本章目录
在频率派统计中，通过将估计器
  
    

    
    
    
    
    
    
    
    
  





  \(\delta\)

应用在某些数据



  \(\mathcal{D}\)

来计算参数估计



  \(\hat{\boldsymbol{\theta}}\)

，因此



  \(\hat{\boldsymbol{\theta}}=δ(\mathcal{D})\)

。 该参数被视为固定的，并且数据被视为随机的，这与贝叶斯方法完全相反。 可以通过计算估计器的采样分布来测量参数估计的不确定性。 为了理解这个概念，想象从一些真实模型



  \(p(·|\boldsymbol{\theta}^*)\)

中采样许多不同的数据集



  \(\mathcal{D}^{(s)}\)

，即让



  \(\mathcal{D}^{(s)}= \left\{x_i^{(s)}\right\}_{i=1}^N\)

，其中



  \(x_i^s \sim p(·|\boldsymbol{\theta}^*)\)

，



  \(\boldsymbol{\theta}^*\)

是真实参数。 这里



  \(s = 1:S\)

已采样数据集的索引，



  \(N\)

是每个这样的数据集的大小。 现在将估计器



  \(\hat{\theta}(·)\)

应用到每个



  \(\mathcal{D}^{(s)}\)

以获得一组估计



  \(\{\hat{\boldsymbol{\theta}}(\mathcal{D}^{(s)})\}\)

。 当我们让



  \(S\to \infty\)

时，在



  \(\hat{\theta}(·)\)

上诱导的分布就是估计器的采样分布。 我们将在后面的章节中讨论使用采样分布的各种方法。 但首先我们描绘了两种计算采样分布本身的方法。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0054/" /><meta property="article:section" content="docs" />
<meta property="article:published_time" content="2019-07-13T20:20:35&#43;08:00" />
<meta property="article:modified_time" content="2019-07-13T20:20:35&#43;08:00" />

<title>6.2 估计器的采样分布 | 学习笔记</title>
<link rel="icon" href="/notes/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/notes/book.min.07a0a866d76192b38577628c20f8f349c797682bd8a0d6b3d18465b8420bd2fb.css" integrity="sha256-B6CoZtdhkrOFd2KMIPjzSceXaCvYoNaz0YRluEIL0vs=">


<script defer src="/notes/cn.search.min.7478184bec8c1596b39dd6644b3d22a4548027de995c3895807c8ed6287aad32.js" integrity="sha256-dHgYS&#43;yMFZazndZkSz0ipFSAJ96ZXDiVgHyO1ih6rTI="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/notes"><img src="/notes/logo.png" alt="Logo" /><span>学习笔记</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>









  










    
    



<ul>
<li><a href="/notes/posts/"><strong>杂事记录</strong></a></li>
<li><a href="/notes/docs/fem/"><strong>有限元法自动求解微分方程</strong></a></li>
<li><a href="/notes/docs/julia/"><strong>基于Julia科学计算</strong></a></li>
<li><a href="/notes/docs/theophy/"><strong>理论物理学习笔记</strong></a></li>
<li><a href="/notes/docs/diffgeo/"><strong>微分几何笔记</strong></a></li>
<li><a href="/notes/docs/mlapp/"><strong>机器学习：概率视角</strong></a>
<ul>
<li><a href="/notes/docs/mlapp/01introduction/">第一章 导论</a></li>
<li><a href="/notes/docs/mlapp/02probability/">第二章 概率</a></li>
<li><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/">第三章 基于离散数据的生成式模型</a></li>
<li><a href="/notes/docs/mlapp/04gaussian_models/">第四章 高斯模型</a></li>
<li><a href="/notes/docs/mlapp/05bayesian_statistics/">第五章 贝叶斯统计</a></li>
<li><a href="/notes/docs/mlapp/06frequentist_statistics/">第六章 频率派统计</a></li>
<li><a href="/notes/docs/mlapp/07linear_regression/">第七章 线性回归</a></li>
</ul>
</li>
<li><a href="/notes/docs/apm/"><strong>主动投资组合管理</strong></a></li>
</ul>












</nav>




  <script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>6.2 估计器的采样分布</strong>

  <label for="toc-control">
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#621-自举bootstrap">6.2.1 自举(Bootstrap)</a></li>
        <li><a href="#622-mle的大样本理论">6.2.2 MLE的大样本理论*</a></li>
      </ul>
    </li>
  </ul>
</nav>


    </aside>
  
 
      </header>

      

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0055/">
    下一页<br>6.3 频率派决策理论
  </a>
  
  
  <a class="older-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0053/">
      上一页<br>6.1 导论
  </a>
  
</nav>

<hr>

<article class="markdown">
  <h1>
    <a href="/notes/docs/mlapp/06frequentist_statistics/0054/">6.2 估计器的采样分布</a>
  </h1>
  

<div>

  <h5>2019-07-13</h5>


<div>

  
    |
    
      <a href="/notes/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
      
    |
  

  
    |
    
      <a href="/notes/tags/%E4%BC%B0%E8%AE%A1%E5%99%A8">估计器</a>
      , 
      <a href="/notes/tags/%E9%87%87%E6%A0%B7%E5%88%86%E5%B8%83">采样分布</a>
      , 
      <a href="/notes/tags/%E8%87%AA%E4%B8%BE">自举</a>
      , 
      <a href="/notes/tags/%E5%A4%A7%E6%A0%B7%E6%9C%AC">大样本</a>
      
    |
  




    <a href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/06frequentist_statistics/0054.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      评论
    </a>


</div>


</div>

<p><a href="/notes/docs/mlapp/06frequentist_statistics/"><strong>返回本章目录</strong></a></p>
<p>在频率派统计中，通过将<strong>估计器</strong>
  
    

    <link rel="stylesheet" href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body,
                {delimiters: [{left: '$$\n', right: '\n$$', display: true}, {left: '$$', right: '$$', display: false}, 
                              {left: '\\[', right: '\\]', display: true}, {left: '\\(', right: '\\)', display: false}]});"></script>
    
  




<span class="katex">
  \(\delta\)
</span>
应用在某些数据


<span class="katex">
  \(\mathcal{D}\)
</span>
来计算参数估计


<span class="katex">
  \(\hat{\boldsymbol{\theta}}\)
</span>
，因此


<span class="katex">
  \(\hat{\boldsymbol{\theta}}=δ(\mathcal{D})\)
</span>
。 该参数被视为固定的，并且数据被视为随机的，这与贝叶斯方法完全相反。 可以通过计算估计器的<strong>采样分布</strong>来测量参数估计的不确定性。 为了理解这个概念，想象从一些真实模型


<span class="katex">
  \(p(·|\boldsymbol{\theta}^*)\)
</span>
中采样许多不同的数据集


<span class="katex">
  \(\mathcal{D}^{(s)}\)
</span>
，即让


<span class="katex">
  \(\mathcal{D}^{(s)}= \left\{x_i^{(s)}\right\}_{i=1}^N\)
</span>
，其中


<span class="katex">
  \(x_i^s \sim p(·|\boldsymbol{\theta}^*)\)
</span>
，


<span class="katex">
  \(\boldsymbol{\theta}^*\)
</span>
是真实参数。 这里


<span class="katex">
  \(s = 1:S\)
</span>
已采样数据集的索引，


<span class="katex">
  \(N\)
</span>
是每个这样的数据集的大小。 现在将估计器


<span class="katex">
  \(\hat{\theta}(·)\)
</span>
应用到每个


<span class="katex">
  \(\mathcal{D}^{(s)}\)
</span>
以获得一组估计


<span class="katex">
  \(\{\hat{\boldsymbol{\theta}}(\mathcal{D}^{(s)})\}\)
</span>
。 当我们让


<span class="katex">
  \(S\to \infty\)
</span>
时，在


<span class="katex">
  \(\hat{\theta}(·)\)
</span>
上诱导的分布就是估计器的采样分布。 我们将在后面的章节中讨论使用采样分布的各种方法。 但首先我们描绘了两种计算采样分布本身的方法。</p>
<h2 id="621-自举bootstrap">6.2.1 自举(Bootstrap)</h2>
<p><img src="" alt="0086.jpg"></p>
<blockquote>
<p>图6.1 对伯努利分布的


<span class="katex">
  \(\hat{\theta}\)
</span>
的采样分布的自举近似。 我们使用


<span class="katex">
  \(B = 10000\)
</span>
个自举样本。 N个数据库由


<span class="katex">
  \({\rm Ber}(\theta= 0.7)\)
</span>
生成。 （a）


<span class="katex">
  \(N = 10\)
</span>
的MLE。（b）


<span class="katex">
  \(N = 100\)
</span>
的MLE。由_bootstrapDemoBer_生成的图。</p>
</blockquote>
<p><strong>自举</strong>(<strong>Bootstrap</strong>)是一种简单的蒙特卡罗技术，用于近似采样分布。 这在估计器是真实参数的复杂函数的情况下特别有用。</p>
<p>这个想法很简单。 如果我们知道真实参数


<span class="katex">
  \(\boldsymbol{\theta}^{\\*}\)
</span>
，我们可以从真实分布生成许多（比如S个）假数据集，每个大小都为


<span class="katex">
  \(N\)
</span>
，即


<span class="katex">
  \(x_i^s \sim p(·|\boldsymbol{\theta}^{\\*}), s=1:S,i=1:N\)
</span>
。然后，我们可以从每个样本计算我们的估计量


<span class="katex">
  \(\hat{\boldsymbol{\theta}}^s= f(x_{1:N}^s)\)
</span>
，并使用所得样本的经验分布作为我们对采样分布的估计。 由于


<span class="katex">
  \(\boldsymbol{\theta}\)
</span>
未知，<strong>参数自举</strong>(<strong>parametric bootstrap</strong>)的思想是使用


<span class="katex">
  \(\hat{\boldsymbol{\theta}}(\mathcal{D})\)
</span>
作为替代来生成样本。 另一种称为<strong>非参数自举</strong>(<strong>non-parametric bootstrap</strong>)的方法是从原始数据


<span class="katex">
  \(\mathcal{D}\)
</span>
中采样


<span class="katex">
  \(x_i^s\)
</span>
（替换），然后像以前一样计算诱导分布。 在（Kleiner等人，2011）中讨论了一些在应用于海量数据集时加速自举的方法。</p>
<p>图6.1展示了一个示例，我们使用参数自举来计算伯努利的MLE采样分布。 （使用非参数自举的结果基本相同。）当N = 10时，我们看到采样分布是不对称的，因此离高斯相当远; 当N = 100时，分布看起来更高斯，正如理论所暗示的那样（见下文）。</p>
<p>一个自然的问题是：由自举计算的参数估计值


<span class="katex">
  \(\hat{\boldsymbol{\theta}}^s=\hat{\boldsymbol{\theta}}(x_{1:N}^s)\)
</span>
与从后验采样的参数值


<span class="katex">
  \(\boldsymbol{\theta}^s \sim p(·| \mathcal{D})\)
</span>
之间的关系是什么？ 从概念上讲，它们完全不同。 但在常见的情况下，先验不是很强，它们可能非常相似。 例如，图6.1（c-d）展现的一个例子，我们使用均匀的


<span class="katex">
  \({\rm Beta}(1,1)\)
</span>
先验计算后验，然后从中进行采样。 我们看到后验和采样分布非常相似。 因此，人们可以将自举分布视为“穷人”的后验; 有关详细信息，请参阅（Hastie等，2001，第235页）。</p>
<p>然而，也许令人惊讶的是，自举可能比后验采样慢。 原因是自举必须拟合模型S次，而在后验采样中，我们通常只拟合模型一次（找到局部众数），然后围绕众数进行局部探索。 这种局部探索通常比从头开始拟合模型快得多。</p>
<h2 id="622-mle的大样本理论">6.2.2 MLE的大样本理论*</h2>
<p>在某些情况下，可以分析地计算某些估计器的采样分布。 特别是，可以证明，在某些条件下，当样本大小趋于无穷大时，MLE的采样分布变为高斯分布。 非正式地，对此结果的要求是模型中的每个参数都“看到”无限量的数据，并且模型是可识别的。 不幸的是，这排除了许多对机器学习感兴趣的模型。 然而，让我们假设我们处于定理所在的简单环境中。</p>
<p>高斯的中心将是MLE 


<span class="katex">
  \(\hat{\boldsymbol{\theta}}\)
</span>
。 但是这个高斯的方差呢？ 直观地，估计器的方差将（相反地）与可能性表面在其峰值处的曲率量相关。 如果曲率很大，峰值将“尖锐”，方差低; 在这种情况下，估计是“很好地确定”。 相反，如果曲率很小，峰值将几乎“平坦”，因此方差很高。</p>
<p>现在让我们正式化这种直觉。 将<strong>得分函数</strong>定义为在某点


<span class="katex">
  \(\hat{\boldsymbol{\theta}}\)
</span>
计算的对数似然梯度：</p>



<span class="katex">
  \[
s(\hat{\boldsymbol{\theta}})\overset{\Delta}{=}\left.\nabla \log p(\mathcal{D}|\boldsymbol{\theta})\right|_{\hat{\boldsymbol{\theta}}} \tag{6.1}
\]
</span>

<p>将<strong>观察到的信息矩阵</strong>(<strong>observed information matrix</strong>)定义为得分函数的负梯度，或等效地，NLL的Hessian：</p>



<span class="katex">
  \[
\boldsymbol{J}(\hat{\boldsymbol{\theta}}|\mathcal{D})\overset{\Delta}{=}-\nabla s(\hat{\boldsymbol{\theta}})=-\left.\nabla_{\boldsymbol{\theta}}^2 \log p(\mathcal{D}|\boldsymbol{\theta})\right|_{\hat{\boldsymbol{\theta}}} \tag{6.2}
\]
</span>

<p>在1维情况下, 上式变成</p>



<span class="katex">
  \[
J(\hat{\theta}|\mathcal{D})=-\left.\dfrac{d^2}{d\theta^2} \log p(\mathcal{D}|\theta)\right|_{\hat{\theta}} \tag{6.3}
\]
</span>

<p>这只是


<span class="katex">
  \(\hat{\theta}\)
</span>
处对数似然函数曲率的度量。</p>
<p>由于我们正在研究采样分布，因此


<span class="katex">
  \(\mathcal{D} =(\boldsymbol{x}_1,\dots,\boldsymbol{x}_N)\)
</span>
 是一组随机变量。 <strong>Fisher信息矩阵</strong>被定义为观察到的信息矩阵的预期值：</p>



<span class="katex">
  \[
\boldsymbol{I}_N(\hat{\boldsymbol{\theta}|\boldsymbol{\theta}^{\\*}})\overset{\Delta}{=}\mathbb{E}_{\boldsymbol{\theta}^{\\*}}\left[\boldsymbol{J}(\hat{\boldsymbol{\theta}}|\mathcal{D})\right] \tag{6.4}
\]
</span>

<p>其中


<span class="katex">
  \(\mathbb{E}_{\boldsymbol{\theta}^{\\*}} [\boldsymbol{f}(\mathcal{D})]\overset{\Delta}{=} \frac{1}{N}\sum_{i=1}^N{ \boldsymbol{f}(\boldsymbol{x}_i)p(\boldsymbol{x}_i | \boldsymbol{\theta}^{\\*})}\)
</span>
 是当应用于从


<span class="katex">
  \(\boldsymbol{\theta}^{\\*}\)
</span>
采样的数据的函数


<span class="katex">
  \(\boldsymbol{f}\)
</span>
的期望值。 通常


<span class="katex">
  \(\boldsymbol{\theta}^{\\*}\)
</span>
表示生成数据的“真实参数”，被假设为已知，因此我们只写更短的形式


<span class="katex">
  \(\boldsymbol{I}_N(\hat{\boldsymbol{\theta}})\overset{\Delta}{=}\boldsymbol{I}_N(\hat{\boldsymbol{\theta}}|\boldsymbol{\theta}^{\\*})\)
</span>
。 此外，很容易看出


<span class="katex">
  \(\boldsymbol{I}_N(\hat{\boldsymbol{\theta}})=N \boldsymbol{I}_1(\hat{\boldsymbol{\theta}})\)
</span>
，因为大小为N的样本的对数似然仅比大小为1的样本的对数似然的“陡”


<span class="katex">
  \(N\)
</span>
倍。所以我们 可以删除1下标，只写


<span class="katex">
  \(\boldsymbol{I}(\hat{\boldsymbol{\theta}})\overset{\Delta}{=}\boldsymbol{I}_1(\hat{\boldsymbol{\theta}})\)
</span>
。 这是通常使用的符号。</p>
<p>现在让


<span class="katex">
  \(\hat{\boldsymbol{\theta}}\overset{\Delta}{=}\hat{\boldsymbol{\theta}}_{\rm mle}(\mathcal{D})\)
</span>
 为MLE，其中


<span class="katex">
  \(\mathcal{D} \sim \boldsymbol{\theta}^{\\*}\)
</span>
。 可以证明</p>



<span class="katex">
  \[
\hat{\boldsymbol{\theta}} \to \mathcal{N}(\boldsymbol{\theta}^{\\*},\boldsymbol{I}(\boldsymbol{\theta}^{\\*})^{-1}) \tag{6.5}
\]
</span>

<p>当


<span class="katex">
  \(N\to \infty\)
</span>
（参见例如（Rice 1995，p265）作为证据）。 我们说MLE的采样分布是<strong>渐近正态的</strong>(<strong>asymptotically normal</strong>)。</p>
<p>MLE的方差(可用来衡量MLE的信任度)怎么样呢？ 不幸的是，


<span class="katex">
  \(\boldsymbol{\theta}^{\\*}\)
</span>
是未知的，因此我们无法评估采样分布的方差。 但是，我们可以通过用


<span class="katex">
  \(\hat{\boldsymbol{\theta}}\)
</span>
代替


<span class="katex">
  \(\boldsymbol{\theta}^{\\*}\)
</span>
来近似采样分布。 因此，


<span class="katex">
  \(\hat{\theta}_k\)
</span>
的近似<strong>标准误差</strong>(<strong>standard errors</strong> )由下式给出</p>



<span class="katex">
  \[
\hat{\rm se}_k\overset{\Delta}{=}\boldsymbol{I}_N(\hat{\boldsymbol{\theta}})_{kk}^{-\frac{1}{2}} \tag{6.6}
\]
</span>

<p>例如，根据公式5.60，我们知道二项采样模型的Fisher信息是</p>



<span class="katex">
  \[
I(\theta)=\dfrac{1}{\theta(1-\theta)} \tag{6.7}
\]
</span>

<p>因此，MLE的近似标准误差是</p>



<span class="katex">
  \[
\hat{\rm se}_k=\dfrac{1}{\sqrt{I_N(\hat{\theta})}}=\dfrac{1}{\sqrt{N I(\hat{\theta})}}=\sqrt{\dfrac{\hat{\theta}(1-\hat{\theta})}{N}} \tag{6.8}
\]
</span>

<p>其中


<span class="katex">
  \(\hat{\theta}= \frac{1}{N}\sum_i{X_i}\)
</span>
。 将其与公式3.27进行比较，公式3.27是均匀先验下的后验标准差。</p>
<p><a href="/notes/docs/mlapp/06frequentist_statistics/"><strong>返回本章目录</strong></a></p>
<p>​</p></article>

<hr>

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0055/">
    下一页<br>6.3 频率派决策理论
  </a>
  
  
  
  <a class="older-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0053/">
      上一页<br>6.1 导论
  </a>
  
</nav>

 

      <footer class="book-footer">
        
  <div class="flex justify-between">





  <div>
    <a class="flex align-center" href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/06frequentist_statistics/0054.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      <span>评论</span>
    </a>
  </div>
  
  
  <div>
    <a class="flex align-center" href="https://gitee.com/-/ide/project/chaoskey/notes/edit/master/-/content/docs/mlapp/06frequentist_statistics/0054.md" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        
  
  <div class="book-comments">

</div>
  
  
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#621-自举bootstrap">6.2.1 自举(Bootstrap)</a></li>
        <li><a href="#622-mle的大样本理论">6.2.2 MLE的大样本理论*</a></li>
      </ul>
    </li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  
</body>

</html>












