<!DOCTYPE html>
<html lang="cn">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="6.4 估计器的理想属性"><meta property="og:title" content="6.4 估计器的理想属性" />
<meta property="og:description" content="返回本章目录
由于频率派决策理论没有提供选择最佳估计器的自动方法，我们需要提出其他启发式方法来选择它们。 在本节中，我们将讨论我们所希望估计器应该具有的一些属性。 不幸的是，我们将看到我们无法同时实现所有这些属性。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0056/" />
<meta property="article:published_time" content="2019-07-15T20:20:35+08:00" />
<meta property="article:modified_time" content="2019-07-15T20:20:35+08:00" />
<title>6.4 估计器的理想属性 | 学习笔记</title>
<link rel="icon" href="/notes/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/notes/book.min.3a0afab795624b6557eb098eddc1c880180827e07fdb828f747c01148048289e.css" integrity="sha256-Ogr6t5ViS2VX6wmO3cHIgBgIJ&#43;B/24KPdHwBFIBIKJ4=">


<script defer src="/notes/cn.search.min.d1c6aa77add781ac6d33c28dc2572e4eda52fc8fd7f5a629c4ac0c43eaf39ed2.js" integrity="sha256-0caqd63XgaxtM8KNwlcuTtpS/I/X9aYpxKwMQ&#43;rzntI="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/notes"><img src="/notes/logo.png" alt="Logo" /><span>学习笔记</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>









  










    
    



<ul>
<li><a href="/notes/posts/"><strong>站点维护记录</strong></a></li>
<li><a href="/notes/docs/fem/"><strong>有限元法自动求解微分方程</strong></a></li>
<li><a href="/notes/docs/julia/"><strong>基于Julia科学计算</strong></a></li>
<li><a href="/notes/docs/theophy/"><strong>理论物理学习笔记</strong></a></li>
<li><a href="/notes/docs/diffgeo/"><strong>微分几何笔记</strong></a></li>
<li><a href="/notes/docs/mlapp/"><strong>机器学习：概率视角</strong></a>
<ul>
<li><a href="/notes/docs/mlapp/01introduction/">第一章 导论</a></li>
<li><a href="/notes/docs/mlapp/02probability/">第二章 概率</a></li>
<li><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/">第三章 基于离散数据的生成式模型</a></li>
<li><a href="/notes/docs/mlapp/04gaussian_models/">第四章 高斯模型</a></li>
<li><a href="/notes/docs/mlapp/05bayesian_statistics/">第五章 贝叶斯统计</a></li>
<li><a href="/notes/docs/mlapp/06frequentist_statistics/">第六章 频率派统计</a></li>
<li><a href="/notes/docs/mlapp/07linear_regression/">第七章 线性回归</a></li>
</ul>
</li>
<li><a href="/notes/docs/apm/"><strong>主动投资组合管理</strong></a></li>
</ul>












</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>6.4 估计器的理想属性</strong>

  <label for="toc-control">
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#641-一致估计器consistent-estimators">6.4.1 一致估计器(Consistent estimators)</a></li>
        <li><a href="#642-无偏估计器unbiased-estimators">6.4.2 无偏估计器(Unbiased estimators)</a></li>
        <li><a href="#643-最小方差估计器minimum-variance-estimators">6.4.3 最小方差估计器(Minimum variance estimators)</a></li>
        <li><a href="#644-偏差-方差权衡">6.4.4 偏差-方差权衡</a>
          <ul>
            <li><a href="#6441-示例估计高斯均值">6.4.4.1 示例：估计高斯均值</a></li>
            <li><a href="#6442-示例岭回归ridge-regression">6.4.4.2 示例：岭回归(ridge regression)</a></li>
            <li><a href="#6443-分类的偏差-方差权衡">6.4.4.3 分类的偏差-方差权衡</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


    </aside>
  
 
      </header>

      

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0058/">
    下一页<br>6.6 频率派统计的病态*
  </a>
  
  
  <a class="older-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0055/">
      上一页<br>6.3 频率派决策理论
  </a>
  
</nav>

<hr>

<article class="markdown">
  <h1>
    <a href="/notes/docs/mlapp/06frequentist_statistics/0056/">6.4 估计器的理想属性</a>
  </h1>
  

<div>

  <h5>2019-07-15</h5>


<div>

  
    |
    
      <a href="/notes/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
      
    |
  

  
    |
    
      <a href="/notes/tags/%E4%BC%B0%E8%AE%A1%E5%99%A8">估计器</a>
      , 
      <a href="/notes/tags/%E5%81%8F%E5%B7%AE">偏差</a>
      , 
      <a href="/notes/tags/%E6%96%B9%E5%B7%AE">方差</a>
      , 
      <a href="/notes/tags/%E5%9D%87%E5%80%BC">均值</a>
      , 
      <a href="/notes/tags/%E5%B2%AD%E5%9B%9E%E5%BD%92">岭回归</a>
      
    |
  




    <a href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/06frequentist_statistics/0056.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      评论
    </a>


</div>


</div>

<p><a href="/notes/docs/mlapp/06frequentist_statistics/"><strong>返回本章目录</strong></a></p>
<p>由于频率派决策理论没有提供选择最佳估计器的自动方法，我们需要提出其他启发式方法来选择它们。 在本节中，我们将讨论我们所希望估计器应该具有的一些属性。 不幸的是，我们将看到我们无法同时实现所有这些属性。</p>
<h2 id="641-一致估计器consistent-estimators">6.4.1 一致估计器(Consistent estimators)</h2>
<p>如果随着样本大小趋于无穷大而最终恢复生成数据的真实参数，即
  
    

    <link rel="stylesheet" href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body,
                {delimiters: [{left: '$$\n', right: '\n$$', display: true}, {left: '$$', right: '$$', display: false}, 
                              {left: '\\[', right: '\\]', display: true}, {left: '\\(', right: '\\)', display: false}]});"></script>
    
  




<span class="katex">
  \(| \mathcal{D}|\to \infty \Rightarrow \hat{\theta}(\mathcal{D}) \to \theta^{\\*}\)
</span>
 （单线箭头表示概率收敛），那么该估计器被称为<strong>一致的</strong>。 当然，这个概念只有在数据实际来自具有参数


<span class="katex">
  \(\theta^{\\*}\)
</span>
的指定模型时才有意义，而实际数据通常不是这种情况。 然而，它可能是一个有用的理论属性。</p>
<p>可以证明MLE是一致估计器。 直观的原因是最大似然相当于最小化


<span class="katex">
  \(\mathbb{KL}(p(\cdot|\boldsymbol{\theta}^{\\*})| p(\cdot|\hat{\boldsymbol{\theta}}))\)
</span>
，其中


<span class="katex">
  \(p(\cdot|\boldsymbol{\theta}^{\\*})\)
</span>
是真实分布，


<span class="katex">
  \(p(\cdot|\hat{\boldsymbol{\theta}})\)
</span>
是我们的估计。 我们可以实现0KL散度，当且仅当


<span class="katex">
  \(\hat{\boldsymbol{\theta}}=\boldsymbol{\theta}^{\\*}\)
</span>
。</p>
<h2 id="642-无偏估计器unbiased-estimators">6.4.2 无偏估计器(Unbiased estimators)</h2>
<p>估计器的<strong>偏差</strong>(<strong>bias</strong>)定义为</p>



<span class="katex">
  \[
{\rm bias}(\hat{\theta}(\cdot))=\mathbb{E}_{p(\mathcal{D}|\theta_{\\*})}\left[\hat{\theta}(\mathcal{D})-\theta_{\\*}\right] \tag{6.32}
\]
</span>

<p>其中


<span class="katex">
  \(\theta_{\\*}\)
</span>
 是真实参数值。 如果偏差为零，则估计器被称为<strong>无偏的</strong>(<strong>unbiased</strong>)。 这意味着采样分布以真实参数为中心。 例如，高斯均值的MLE是无偏的：</p>



<span class="katex">
  \[
{\rm bias}(\hat{\mu})=\mathbb{E}\left[\bar{x}\right]-\mu=\mathbb{E}\left[\dfrac{1}{N}\sum_{i=1}^N{x_i}\right]-\mu=\dfrac{N \mu}{N}-\mu=0 \tag{6.33}
\]
</span>

<p>然而，高斯方差的MLE


<span class="katex">
  \(\hat{\sigma}^2\)
</span>
不是


<span class="katex">
  \(\sigma^2\)
</span>
的无偏估计。 事实上，可以证明（练习6.3）</p>



<span class="katex">
  \[
\mathbb{E}\left[\hat{\sigma}^2\right]=\dfrac{N-1}{N}\sigma^2 \tag{6.34}
\]
</span>

<p>但是，下面这个估计器</p>



<span class="katex">
  \[
\hat{\sigma}_{N-1}^2=\dfrac{N}{N-1}\hat{\sigma}^2=\dfrac{1}{N-1}\sum_{i=1}^N{(x_i-\bar{x})^2}  \tag{6.35}
\]
</span>

<p>是无偏估计器，我们很容易证明如下:</p>



<span class="katex">
  \[
\mathbb{E}\left[\hat{\sigma}_{N-1}^2\right]=\mathbb{E}\left[\dfrac{N}{N-1}\hat{\sigma}^2\right]=\dfrac{N}{N-1}\dfrac{N-1}{N}\sigma^2=\sigma^2  \tag{6.36}
\]
</span>

<p>在Matlab中，var(X)返回


<span class="katex">
  \(\hat{\sigma}_{N-1}^2\)
</span>
，而var(X,1)返回


<span class="katex">
  \(\sigma^2\)
</span>
（MLE）。 对于足够大的N，差异可以忽略不计。</p>
<p>尽管MLE有时可能是一个有偏差的估计器，但人们可以渐近地认为它总是无偏见的。 （这对于MLE是一致估计器来说是必要的。）</p>
<p>虽然无偏听起来像一个理想的属性，但并非总是如此。 有关这一点的讨论，请参见第6.4.4节和（Lindley 1972）。</p>
<h2 id="643-最小方差估计器minimum-variance-estimators">6.4.3 最小方差估计器(Minimum variance estimators)</h2>
<p>我们希望我们的估算器是无偏的，似乎在直觉上是合理的（尽管我们将在下面提出一些反对这一主张的论据）。 但是，无偏是不够的。 例如，假设我们想要从


<span class="katex">
  \(\mathcal{D} = \{x_1,\dots,x_N\}\)
</span>
估计高斯均值。 仅查看第一个数据点


<span class="katex">
  \(\hat{\theta}(\mathcal{D})= x_1\)
</span>
的估计器是无偏估计器，但通常比经验均值


<span class="katex">
  \(\bar{x}\)
</span>
（也是无偏的）更远离


<span class="katex">
  \(\theta_{\\*}\)
</span>
。 因此估计器的方差也很重要。</p>
<p>一个自然的问题是：方差可以持续多久？(how long can the variance go?) 一个著名的结果，称为<strong>Cramer-Rao下界</strong>，提供了无偏估计器的方差下界。 更确切地说，</p>
<p><strong>定理6.4.1.</strong>（Cramer-Rao不等式） 令


<span class="katex">
  \(X_1,\dots,X_n \sim p(X |\theta_0)\)
</span>
, 并且


<span class="katex">
  \(\hat{\theta}=\hat{\theta}(x_1,\dots,x_n)\)
</span>
是


<span class="katex">
  \(\theta_0\)
</span>
的无偏估计器。 那么，在


<span class="katex">
  \(p(X |\theta_0)\)
</span>
的各种平滑假设下，我们有</p>



<span class="katex">
  \[
{\rm var}\left[\hat{\theta}\right] \ge \dfrac{1}{n I(\theta_0)}  \tag{6.37}
\]
</span>

<p>其中


<span class="katex">
  \(I(\theta_0)\)
</span>
是Fisher信息矩阵 (参见第6.2.2节)。</p>
<p>证明可以在比如（Rice 1995，p275）中找到。</p>
<p>可以证明MLE达到了CramerRao下界，因此具有任何无偏估计器的最小渐近方差。 因此，MLE被认为是<strong>渐近最优的</strong>。</p>
<h2 id="644-偏差-方差权衡">6.4.4 偏差-方差权衡</h2>
<p>虽然使用无偏估计器似乎是一个好主意，但情况并非总是如此。 为了解原因，假设我们使用二次损失。 如上所示，相应的风险是MSE。 我们现在推导一个非常有用的MSE分解。 （所有的期望和方差都是关于真正的分布


<span class="katex">
  \(p(\mathcal{D} |\theta^{\\*})\)
</span>
的，但是为了符号简洁我们放弃了显式条件。）设


<span class="katex">
  \(\hat{\theta}=\hat{\theta}(\mathcal{D})\)
</span>
表示估计，


<span class="katex">
  \(\bar{\theta}=\mathbb{E}\left[\hat{\theta}\right]\)
</span>
表示估计的期望值。 估计（会随


<span class="katex">
  \(\mathcal{D}\)
</span>
而变）。 于是我们有</p>



<span class="katex">
  \[
\begin{aligned}
\mathbb{E}\left[(\hat{\theta}-\theta^{*})^2\right]=& \mathbb{E}\left[\left[(\hat{\theta}-\bar{\theta})+(\bar{\theta}-\theta^{*})\right]^2\right]  \\
\quad =& \mathbb{E}\left[(\hat{\theta}-\bar{\theta})^2\right]+2(\bar{\theta}-\theta^{*})\mathbb{E}\left[(\hat{\theta}-\bar{\theta})\right]+(\bar{\theta}-\theta^{*})^2   \\
\quad =& \mathbb{E}\left[(\hat{\theta}-\bar{\theta})^2\right]+(\bar{\theta}-\theta^{*})^2  \\
\quad =& {\rm var}\left[\hat{\theta}\right]+{\rm bias}^2(\hat{\theta})  
\end{aligned} \tag{6.38-41}
\]
</span>

<p>用文字表述为:</p>



<span class="katex">
  \[
\boxed{{\rm MSE}={\rm variance}+{\rm bias}^2} \tag{6.42}
\]
</span>

<p>这称为<strong>偏差-方差权衡</strong>（参见例如（Geman等人，1992））。 这意味着使用偏差估计器可能是明智的，只要它减少我们的方差，进而假设我们的目标是最小化平方误差。</p>
<h3 id="6441-示例估计高斯均值">6.4.4.1 示例：估计高斯均值</h3>
<p>让我们举一个例子，基于（Hoff 2009，p79）。 假如我们想要从


<span class="katex">
  \(\boldsymbol{x} =(x_1,\dots,x_N)\)
</span>
估计高斯均值。 我们假设数据是从


<span class="katex">
  \(x_i \sim \mathcal{N}(\theta^{\\*} = 1，σ2)\)
</span>
中采样的。 一个明显的估计是MLE。 它的偏差为0，方差为</p>



<span class="katex">
  \[
{\rm var}[\bar{x}|\theta^{\\*}]=\dfrac{\sigma^2}{N}  \tag{6.43}
\]
</span>

<p>但我们也可以使用MAP估计。 在4.6.1节中，我们证明了在


<span class="katex">
  \(\mathcal{N}(\theta_0,\sigma^2/\kappa_0)\)
</span>
形式的高斯先验下的MAP估计由下式给出：</p>



<span class="katex">
  \[
\tilde{x}\overset{\Delta}{=}\dfrac{N}{N+\kappa_0}\bar{x}+\dfrac{\kappa_0}{N+\kappa_0}\theta_0=w \bar{x}+(1-w)\theta_0  \tag{6.44}
\]
</span>

<p>其中


<span class="katex">
  \(0 \le w \le 1\)
</span>
控制我们相信MLE与先验相比的程度。 （这也是后验均值，因为高斯的均值和众数是相同的。）偏差和方差由下式给出：</p>



<span class="katex">
  \[
\begin{aligned}
\mathbb{E}[\tilde{x}]-\theta^{*}=&w \theta^{*}+(1-w)
\theta_0-\theta^{*}=(1-w)(\theta_0-\theta^{*})    \\
{\rm var}[\tilde{x}]=& w^2\dfrac{\sigma^2}{N} 
\end{aligned} \tag{6.45-46}
\]
</span>

<p>因此，虽然MAP估计是有偏的（假设w &lt;1），但它具有较低的方差。</p>
<p><img src="" alt="0089.jpg"></p>
<blockquote>
<p>图6.4 左：采用不同先验强度


<span class="katex">
  \(\kappa_0\)
</span>
的MAP估计的采样分布。 （MLE对应于


<span class="katex">
  \(\kappa_0=0\)
</span>
.）右：相对于不同样本大小的MLE的MSE。 基于图5.6（Hoff 2009）。 由_samplingDistGaussShrinkage_生成的图。</p>
</blockquote>
<p><img src="" alt="0090.jpg"></p>
<blockquote>
<p>图6.5 岭回归的偏差-方差权衡的描述。 我们从真实函数生成100个数据集，以纯绿色显示。 左图：我们绘制了20个不同数据集的正则拟合。 我们使用具有高斯RBF展开的线性回归，其中25个中心均匀地分布在[0,1]间隔上。 右图：我们绘制拟合的平均值，对所有100个数据集求平均值。 顶行：强正规化：我们看到个体拟合彼此相似（低方差），但平均值远非事实（高偏差）。 底行：轻度正则化：我们看到个体拟合彼此非常不同（高方差），但平均值接近事实（低偏差）。 基于（Bishop 2006a）图3.5。 由biasVarModelComplexity3生成的图。</p>
</blockquote>
<p>让我们假设我们先验的略有错误指定，因此我们使用


<span class="katex">
  \(\theta_0= 0\)
</span>
，而事实是


<span class="katex">
  \(\theta^{\\*} = 1\)
</span>
.在图6.4（a）中，我们看到对


<span class="katex">
  \(\kappa_0> 0\)
</span>
的MAP估计的采样分布偏离事实，但比MLE有更小的方差（较窄）。</p>
<p>在图6.4（b）中，我们绘制了


<span class="katex">
  \({\rm mse}(\tilde{x})/{\rm mse}(x) \ {\rm v.s.} \ N\)
</span>
 。我们看到MAP的估计比MLE有更低的MSE，特别是对于小样本(


<span class="katex">
  \(\kappa_0 \in \{1,2\}\)
</span>
)。 


<span class="katex">
  \(\kappa_0= 0\)
</span>
情况下对应于MLE，


<span class="katex">
  \(\kappa_0= 2\)
</span>
情况对应于强先验，这会损害性能，因为先验均值是错误的。 “调整”先验的强度显然很重要，这是我们稍后讨论的一个主题。</p>
<h3 id="6442-示例岭回归ridge-regression">6.4.4.2 示例：岭回归(ridge regression)</h3>
<p>偏差方差权衡的另一个重要例子出现在岭回归中，我们将在7.5节中讨论。 简而言之，这对应于在高斯先验


<span class="katex">
  \(p(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w} | 0,\lambda^{-1}\boldsymbol{I})\)
</span>
下的线性回归的MAP估计。这个零均值先验鼓励小权重，这减少了过度拟合; 精度项


<span class="katex">
  \(\lambda\)
</span>
控制该先验的强度。 使用


<span class="katex">
  \(\lambda= 0\)
</span>
导致MLE; 使用


<span class="katex">
  \(\lambda> 0\)
</span>
会导致有偏估计。 为了说明对方差的影响，请考虑一个简单的例子。 图6.5的左边绘制了每条拟合曲线，右边绘制了平均拟合曲线。 我们看到，随着我们增加正则化器的强度，方差减小，但偏差增加。</p>
<h3 id="6443-分类的偏差-方差权衡">6.4.4.3 分类的偏差-方差权衡</h3>
<p>如果我们使用0-1损失而不是平方误差，则上述分析会中断，因为频率派风险不再表示为平方偏差加方差。 事实上，可以证明（（Hastie et al.2009）的习题7.2）变成了偏差和方差相乘。 如果估计值位于决策边界的正确一侧，则偏差为负，减小方差将降低误分类率。 但如果估计是在决策边界的错误一侧，则偏差是正的，因此增加方差是值得的（Friedman 1997a）。 这个鲜为人知的事实说明偏差-方差权衡对于分类来说并不是非常有用。 最好关注预期损失（见下文），而不是直接关注偏差和方差。 我们可以使用交叉验证来估计预期损失，正如我们在6.5.3节中讨论的那样。</p>
<p><a href="/notes/docs/mlapp/06frequentist_statistics/"><strong>返回本章目录</strong></a></p></article>

<hr>

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0058/">
    下一页<br>6.6 频率派统计的病态*
  </a>
  
  
  
  <a class="older-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/06frequentist_statistics/0055/">
      上一页<br>6.3 频率派决策理论
  </a>
  
</nav>

 

      <footer class="book-footer">
        
  <div class="flex justify-between">





  <div>
    <a class="flex align-center" href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/06frequentist_statistics/0056.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      <span>评论</span>
    </a>
  </div>
  
  
  <div>
    <a class="flex align-center" href="https://gitee.com/-/ide/project/chaoskey/notes/edit/master/-/content/docs/mlapp/06frequentist_statistics/0056.md" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        
  
  <div class="book-comments">

</div>
  
  
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#641-一致估计器consistent-estimators">6.4.1 一致估计器(Consistent estimators)</a></li>
        <li><a href="#642-无偏估计器unbiased-estimators">6.4.2 无偏估计器(Unbiased estimators)</a></li>
        <li><a href="#643-最小方差估计器minimum-variance-estimators">6.4.3 最小方差估计器(Minimum variance estimators)</a></li>
        <li><a href="#644-偏差-方差权衡">6.4.4 偏差-方差权衡</a>
          <ul>
            <li><a href="#6441-示例估计高斯均值">6.4.4.1 示例：估计高斯均值</a></li>
            <li><a href="#6442-示例岭回归ridge-regression">6.4.4.2 示例：岭回归(ridge regression)</a></li>
            <li><a href="#6443-分类的偏差-方差权衡">6.4.4.3 分类的偏差-方差权衡</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  
</body>

</html>












