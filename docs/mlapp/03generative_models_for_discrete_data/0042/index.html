<!DOCTYPE html>
<html lang="cn">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="3.4 狄利克雷-多项模型"><meta property="og:title" content="3.4 狄利克雷-多项模型" />
<meta property="og:description" content="返回本章目录
在上一节中，我们讨论了如何推断硬币&quot;正面&quot;出现的概率。在本节中，我们推广这些结果，以推断出有
  
    

    
    
    
    
    
    
    
    
  





  \(K\)

面的骰子出现第



  \(k\)

面的概率。这似乎是另一种玩具练习，但将在后面看到, 我们的研究方法将被广泛用于分析文本数据，生物序列数据等。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chaoskey.gitee.io/notes/docs/mlapp/03generative_models_for_discrete_data/0042/" />
<meta property="article:published_time" content="2019-06-30T20:20:35+08:00" />
<meta property="article:modified_time" content="2019-06-30T20:20:35+08:00" />
<title>3.4 狄利克雷-多项模型 | 学习笔记</title>
<link rel="icon" href="/notes/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/notes/book.min.3a0afab795624b6557eb098eddc1c880180827e07fdb828f747c01148048289e.css" integrity="sha256-Ogr6t5ViS2VX6wmO3cHIgBgIJ&#43;B/24KPdHwBFIBIKJ4=">


<script defer src="/notes/cn.search.min.d1c6aa77add781ac6d33c28dc2572e4eda52fc8fd7f5a629c4ac0c43eaf39ed2.js" integrity="sha256-0caqd63XgaxtM8KNwlcuTtpS/I/X9aYpxKwMQ&#43;rzntI="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/notes"><img src="/notes/logo.png" alt="Logo" /><span>学习笔记</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>









  










    
    



<ul>
<li><a href="/notes/posts/"><strong>站点维护记录</strong></a></li>
<li><a href="/notes/docs/fem/"><strong>有限元法自动求解微分方程</strong></a></li>
<li><a href="/notes/docs/julia/"><strong>基于Julia科学计算</strong></a></li>
<li><a href="/notes/docs/theophy/"><strong>理论物理学习笔记</strong></a></li>
<li><a href="/notes/docs/diffgeo/"><strong>微分几何笔记</strong></a></li>
<li><a href="/notes/docs/mlapp/"><strong>机器学习：概率视角</strong></a>
<ul>
<li><a href="/notes/docs/mlapp/01introduction/">第一章 导论</a></li>
<li><a href="/notes/docs/mlapp/02probability/">第二章 概率</a></li>
<li><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/">第三章 基于离散数据的生成式模型</a></li>
<li><a href="/notes/docs/mlapp/04gaussian_models/">第四章 高斯模型</a></li>
<li><a href="/notes/docs/mlapp/05bayesian_statistics/">第五章 贝叶斯统计</a></li>
<li><a href="/notes/docs/mlapp/06frequentist_statistics/">第六章 频率派统计</a></li>
<li><a href="/notes/docs/mlapp/07linear_regression/">第七章 线性回归</a></li>
</ul>
</li>
<li><a href="/notes/docs/apm/"><strong>主动投资组合管理</strong></a></li>
</ul>












</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>3.4 狄利克雷-多项模型</strong>

  <label for="toc-control">
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#341-拟然">3.4.1 拟然</a></li>
        <li><a href="#342-先验">3.4.2 先验</a></li>
        <li><a href="#343-后验">3.4.3 后验</a></li>
        <li><a href="#344-后验预测">3.4.4 后验预测</a>
          <ul>
            <li><a href="#3441-工作示例使用词袋的语言模型">3.4.4.1 工作示例：使用词袋的语言模型</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


    </aside>
  
 
      </header>

      

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/03generative_models_for_discrete_data/0043/">
    下一页<br>3.5 朴素贝叶斯分类器
  </a>
  
  
  <a class="older-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/03generative_models_for_discrete_data/0041/">
      上一页<br>3.3 贝塔-二项模型
  </a>
  
</nav>

<hr>

<article class="markdown">
  <h1>
    <a href="/notes/docs/mlapp/03generative_models_for_discrete_data/0042/">3.4 狄利克雷-多项模型</a>
  </h1>
  

<div>

  <h5>2019-06-30</h5>


<div>

  
    |
    
      <a href="/notes/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
      
    |
  

  
    |
    
      <a href="/notes/tags/%E6%8B%9F%E7%84%B6">拟然</a>
      , 
      <a href="/notes/tags/%E5%85%88%E9%AA%8C">先验</a>
      , 
      <a href="/notes/tags/%E5%90%8E%E9%AA%8C">后验</a>
      , 
      <a href="/notes/tags/%E9%A2%84%E6%B5%8B">预测</a>
      
    |
  




    <a href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/03generative_models_for_discrete_data/0042.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      评论
    </a>


</div>


</div>

<p><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/"><strong>返回本章目录</strong></a></p>
<p>在上一节中，我们讨论了如何推断硬币&quot;正面&quot;出现的概率。在本节中，我们推广这些结果，以推断出有
  
    

    <link rel="stylesheet" href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body,
                {delimiters: [{left: '$$\n', right: '\n$$', display: true}, {left: '$$', right: '$$', display: false}, 
                              {left: '\\[', right: '\\]', display: true}, {left: '\\(', right: '\\)', display: false}]});"></script>
    
  




<span class="katex">
  \(K\)
</span>
面的骰子出现第


<span class="katex">
  \(k\)
</span>
面的概率。这似乎是另一种玩具练习，但将在后面看到, 我们的研究方法将被广泛用于分析文本数据，生物序列数据等。</p>
<h2 id="341-拟然">3.4.1 拟然</h2>
<p>我们观察丢


<span class="katex">
  \(N\)
</span>
次骰子，


<span class="katex">
  \(\mathcal{D}=\{x_1,\dots,x_N\}\)
</span>
 ，其中


<span class="katex">
  \(x_i \in \{1,\dots,K\}\)
</span>
。如果假设数据是i.i.d的，那么似然形如下式:</p>



<span class="katex">
  \[
p(\mathcal{D} | \boldsymbol{\theta})= \prod_{k=1}^K {\theta_k^{N_k}} \tag{3.36}
\]
</span>

<p>其中


<span class="katex">
  \(N_k=\sum_{i=1}^N {\mathbb{I}(x_i=k)}\)
</span>
是事件


<span class="katex">
  \(k\)
</span>
发生的次数（这是该模型的充分统计）。多项模型的拟然具有相同的形式，只相差一个不相关的常数因子。</p>
<p><img src="" alt="0063.jpg"></p>
<blockquote>
<p>图3.7 （a）在看到


<span class="katex">
  \(N_1=3,N_0=17\)
</span>
后的后验预测。（b）绘制了使用MAP的插入近似。</p>
</blockquote>
<h2 id="342-先验">3.4.2 先验</h2>
<p>由于参数向量落在一个K维概率单纯形中，我们需要支持这个单纯形的先验，理想情况下它也是共轭的。幸运的是，狄利克雷分布（第2.5.4节）满足两个标准。所以我们将使用如下先验：</p>



<span class="katex">
  \[
{\rm Dir}(\boldsymbol{\theta} | \alpha)=\dfrac{1}{B(\alpha)}\prod_{k=1}^K {\theta_k^{\alpha_k - 1} \mathbb{I}(\boldsymbol{\theta} \in S_K)} \tag{3.37}
\]
</span>

<h2 id="343-后验">3.4.3 后验</h2>
<p>将上面的拟然乘上先验得到的后验, 也是狄利克雷分布分布:</p>



<span class="katex">
  \[
\begin{aligned}
p(\boldsymbol{\theta} | \mathcal{D}) & \propto p(\mathcal{D} | \boldsymbol{\theta}) p(\boldsymbol{\theta})  \\
\quad &  \propto \prod_{k=1}^K {\theta_k^{N_k} \  \theta_k^{\alpha_k - 1}} = \prod_{k=1}^K { \theta_k^{\alpha_k + N_k - 1}}   \\
\quad &  = {\rm Dir}(\boldsymbol{\theta} | \alpha_1+N_1,\dots,\alpha_K+N_K)  
\end{aligned} \tag{3.38-40} 
\]
</span>

<p>我们看到，这个后验是通过先验的超参数（伪计数 ）


<span class="katex">
  \(\alpha_k\)
</span>
加上经验计数


<span class="katex">
  \(N_k\)
</span>
而获得的.。</p>
<p>我们可以通过使用微积分推导出该后验的众数（即，MAP估计）， 当然我们必须强制约束


<span class="katex">
  \(\sum_k {\theta_k} = 1\)
</span>
  。我们可以使用拉格朗日乘子做到这一点 。受约束的目标函数，或 拉格朗日量，就是对数似然, 加上对数,再加上约束项:</p>



<span class="katex">
  \[
\ell(\boldsymbol{\theta}, \lambda) = \sum_k \ {N_k {\rm log} \theta_k}+\sum_k \ {(\alpha_k-1) {\rm log} \theta_k} + \lambda \left(1- \sum_k {\theta_k} \right) \tag{3.41}
\]
</span>

<p>为了简化记号, 我们定义


<span class="katex">
  \(N_k^{'} \overset{\Delta}{=}N_k+\alpha_k-1\)
</span>
 。取


<span class="katex">
  \(\lambda\)
</span>
取导数产生原始约束：</p>



<span class="katex">
  \[
\dfrac{\partial \ell}{\partial \lambda} = 1- \sum_k {\theta_k} = 0 \tag{3.42}
\]
</span>

<p>对


<span class="katex">
  \(\theta_k\)
</span>
求导数:</p>



<span class="katex">
  \[
\dfrac{\partial \ell}{\partial \theta_k} = \dfrac{N_k^{'}}{\theta_k}-\lambda=0 \tag{3.43}
\]
</span>




<span class="katex">
  \[
N_k^{'}=\lambda \ \theta_k \tag{3.44}
\]
</span>

<p>利用约束求和可以解出


<span class="katex">
  \(\lambda\)
</span>
:</p>



<span class="katex">
  \[
\sum_k {N_k^{'}} = \lambda \sum_k {\theta_l} \tag{3.45}
\]
</span>




<span class="katex">
  \[
N+\alpha_0-K = \lambda  \tag{3.46}
\]
</span>

<p>其中 ，


<span class="katex">
  \(\alpha_0 \overset{\Delta}{=} \sum_{k=1}^K {\alpha_k}\)
</span>
是先验的等效样本尺寸。因此，MAP估计由下式给出:</p>



<span class="katex">
  \[
\hat{\theta}_k=\dfrac{N_k+\alpha_k-1}{N+\alpha_0-K} \tag{3.47}
\]
</span>

<p>这与公式2.77是一致。如果我们使用均匀先验


<span class="katex">
  \(\alpha_k=1\)
</span>
，我们再次得到类似的MLE:</p>



<span class="katex">
  \[
\hat{\theta}_k=\dfrac{N_k}{N} \tag{3.48}
\]
</span>

<p>这恰好是


<span class="katex">
  \(k\)
</span>
面朝上的经验分式。</p>
<h2 id="344-后验预测">3.4.4 后验预测</h2>
<p>这个后验预测分布, 是基于一个单独的多项试验, 表示如下:</p>



<span class="katex">
  \[
\begin{aligned}
p(X=j|\mathcal{D}) & = \int {p(X=j|\boldsymbol{\theta}) p(\boldsymbol{\theta} | \mathcal{D}) d\boldsymbol{\theta}}   \\
  \quad &  =  \int {p(X=j|\theta_j) \left[\int { p(\boldsymbol{\theta}_{-j},\theta_j | \mathcal{D}) d\boldsymbol{\theta}_{-j}} \right] d \theta_j}    \\
  \quad &  =  \int {\theta_j\ p(\theta_j | \mathcal{D}) d \theta_j}=\mathbb{E}[\theta_j | \mathcal{D}]=\dfrac{\alpha_j+N_j}{\sum_k {(\alpha_k+N_k)}}=\dfrac{\alpha_j+N_j}{\alpha_0+N}
\end{aligned}  \tag{3.49-51} 
\]
</span>

<p>其中


<span class="katex">
  \(\boldsymbol{\theta}_{-j}\)
</span>
表示从参数向量


<span class="katex">
  \(\boldsymbol{\theta}\)
</span>
中剔除


<span class="katex">
  \(\theta_j\)
</span>
后的降1维向量.。参考练习3.13。</p>
<p>正如我们在3.3.4.1节中看到的那样，上面的表达式避免了零计数问题。实际上，这种贝叶斯平滑的形式,在多项情况下比二项的情况更加重要，因为一旦我们开始将数据划分为多个类别, 数据稀疏的拟然会增加。</p>
<h3 id="3441-工作示例使用词袋的语言模型">3.4.4.1 工作示例：使用词袋的语言模型</h3>
<p>使用狄利克雷-多项模型进行贝叶斯平滑的一种应用是<strong>语言建模</strong>，这意味着要预测在一个句子中下个可能出现的单词。在这里，我们将采取一种非常简单的方法，假设第


<span class="katex">
  \(i\)
</span>
个单词


<span class="katex">
  \(X_i \in {1,\dots,K}\)
</span>
简单地独立于所有其他单词, 采用


<span class="katex">
  \({\rm Cat}(\boldsymbol{\theta})\)
</span>
分布。称此为<strong>词袋模型</strong>(bag of words model.)。在获得已经出现的单词序列，我们如何预测下一个可能会出现哪一个？</p>
<p>例如，假设我们观察到如下的序列（儿童童谣的一部分）：</p>
<p>Mary had a little lamb, little lamb, little lamb,</p>
<p>Mary had a little lamb, its fleece as white as snow</p>
<p>此外，假设我们的词汇包括以下词语：</p>
<p>mary lamb little big fleece white black snow rain unk</p>
<p>1 2 3 4 5 6 7 8 9 10</p>
<p>这里<strong>unk</strong>代表未知，表示列表中没有出现的所有其他单词。为了编码童谣的每一行，我们首先去除标点符号，并删除任何<strong>停止词</strong> ，如“a”，“as”，“the”等。我们也可以执行<strong>词干</strong>，这意味着将词语缩减为基本形式, 比如删除复数尾部的s， 或 动词的ing（例如， <em>running</em> 变得 <em>run</em>）。在这个例子中，没有词需要词干。最后，我们将每个单词用词汇表的索引替换：</p>
<p>1 10 3 2 3 2 3 2</p>
<p>1 10 3 2 10 5 10 6 8</p>
<p>我们现在忽略单词次序，并计算每个单词出现的频率，从而产生一个单词计数的直方图：</p>
<p><img src="" alt="0064.jpg"></p>
<p>将上述计数记为


<span class="katex">
  \(N_j\)
</span>
。如果我们对


<span class="katex">
  \(\boldsymbol{\theta}\)
</span>
使用


<span class="katex">
  \({\rm Dir}(\alpha)\)
</span>
先验，那么后验预测就是:</p>



<span class="katex">
  \[
p(\tilde{X}=j | \mathcal{D})=\mathbb{E}[\theta_j | \mathcal{D}] = \dfrac{\alpha_j+N_j}{\sum_{j^{'}} {(\alpha_{j^{'}}+N_{j^{'}})}}= \dfrac{1+N_j}{10+17} \tag{3.52}
\]
</span>

<p>其中第二等式后, 取


<span class="katex">
  \(\alpha_j=1\)
</span>
, 进而得到:</p>



<span class="katex">
  \[
p(\tilde{X}=j | \mathcal{D})=(3/27,5/27,5/27,1/27,2/27,2/27,1/27,2/27,1/27,5/27) \tag{3.53}
\]
</span>

<p>这个模型的预测分布是


<span class="katex">
  \(X = 2\)
</span>
（“lamb”）和 


<span class="katex">
  \(X = 10\)
</span>
(“unk”)。请注意，&ldquo;big”，“black”和“rain”这些词在未来也会以非零概率发生，即使它们以前从未见过。稍后我们将看到更复杂的语言模型。</p>
<p><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/"><strong>返回本章目录</strong></a></p></article>

<hr>

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/03generative_models_for_discrete_data/0043/">
    下一页<br>3.5 朴素贝叶斯分类器
  </a>
  
  
  
  <a class="older-posts" href="https://chaoskey.gitee.io/notes/docs/mlapp/03generative_models_for_discrete_data/0041/">
      上一页<br>3.3 贝塔-二项模型
  </a>
  
</nav>

 

      <footer class="book-footer">
        
  <div class="flex justify-between">





  <div>
    <a class="flex align-center" href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/03generative_models_for_discrete_data/0042.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      <span>评论</span>
    </a>
  </div>
  
  
  <div>
    <a class="flex align-center" href="https://gitee.com/-/ide/project/chaoskey/notes/edit/master/-/content/docs/mlapp/03generative_models_for_discrete_data/0042.md" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        
  
  <div class="book-comments">

</div>
  
  
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#341-拟然">3.4.1 拟然</a></li>
        <li><a href="#342-先验">3.4.2 先验</a></li>
        <li><a href="#343-后验">3.4.3 后验</a></li>
        <li><a href="#344-后验预测">3.4.4 后验预测</a>
          <ul>
            <li><a href="#3441-工作示例使用词袋的语言模型">3.4.4.1 工作示例：使用词袋的语言模型</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  
</body>

</html>












