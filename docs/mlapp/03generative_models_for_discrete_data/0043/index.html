<!DOCTYPE html>
<html lang="cn">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="3.5 朴素贝叶斯分类器"><meta property="og:title" content="3.5 朴素贝叶斯分类器" />
<meta property="og:description" content="返回本章目录
在本节中，我们将讨论如何对离散值特征的向量进行分类，
  
    

    
    
    
    
    
    
    
    
  





  \(\boldsymbol{x} \in {1,\dots,K}^D\)

，其中



  \(K\)

是每个特征的值域数， 



  \(D\)

是特征的数量。我们将使用生成方法。这要求我们指定类条件分布



  \(p(\boldsymbol{x} | y=c)\)

。最简单的方法是假设特征是条件独立的, 对给定类标签。这使我们可以将类条件密度写成一维密度的乘积:




  \[
p(\boldsymbol{x} | y=c, \boldsymbol{\theta}) = \prod_{j=1}^D {p(\boldsymbol{x}_j | y=c, \theta_{jc}) } \tag{3.54}
\]


此模型被称为 朴素贝叶斯分类器 （NBC）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chaoskey.github.io/notes/docs/mlapp/03generative_models_for_discrete_data/0043/" /><meta property="article:section" content="docs" />
<meta property="article:published_time" content="2019-07-01T20:20:35&#43;08:00" />
<meta property="article:modified_time" content="2019-07-01T20:20:35&#43;08:00" />

<title>3.5 朴素贝叶斯分类器 | 学习笔记</title>
<link rel="icon" href="/notes/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/notes/book.min.07a0a866d76192b38577628c20f8f349c797682bd8a0d6b3d18465b8420bd2fb.css" integrity="sha256-B6CoZtdhkrOFd2KMIPjzSceXaCvYoNaz0YRluEIL0vs=">


<script defer src="/notes/cn.search.min.b24a80f802a70544ae3facb372d58456ff935b9a88f45da5f9aad2cba715f951.js" integrity="sha256-skqA&#43;AKnBUSuP6yzctWEVv&#43;TW5qI9F2l&#43;arSy6cV&#43;VE="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/notes"><img src="/notes/logo.png" alt="Logo" /><span>学习笔记</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>









  










    
    



<ul>
<li><a href="/notes/posts/"><strong>杂事记录</strong></a></li>
<li><a href="/notes/docs/fem/"><strong>有限元法自动求解微分方程</strong></a></li>
<li><a href="/notes/docs/julia/"><strong>基于Julia科学计算</strong></a></li>
<li><a href="/notes/docs/theophy/"><strong>理论物理学习笔记</strong></a></li>
<li><a href="/notes/docs/diffgeo/"><strong>微分几何笔记</strong></a></li>
<li><a href="/notes/docs/mlapp/"><strong>机器学习：概率视角</strong></a>
<ul>
<li><a href="/notes/docs/mlapp/01introduction/">第一章 导论</a></li>
<li><a href="/notes/docs/mlapp/02probability/">第二章 概率</a></li>
<li><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/">第三章 基于离散数据的生成式模型</a></li>
<li><a href="/notes/docs/mlapp/04gaussian_models/">第四章 高斯模型</a></li>
<li><a href="/notes/docs/mlapp/05bayesian_statistics/">第五章 贝叶斯统计</a></li>
<li><a href="/notes/docs/mlapp/06frequentist_statistics/">第六章 频率派统计</a></li>
<li><a href="/notes/docs/mlapp/07linear_regression/">第七章 线性回归</a></li>
</ul>
</li>
<li><a href="/notes/docs/apm/"><strong>主动投资组合管理</strong></a></li>
</ul>












</nav>




  <script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>3.5 朴素贝叶斯分类器</strong>

  <label for="toc-control">
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#351-模型拟合">3.5.1 模型拟合</a>
          <ul>
            <li><a href="#3511-nbc的mle">3.5.1.1 NBC的MLE</a></li>
            <li><a href="#3512-朴素贝叶斯的贝叶斯算法bayesian-naive-bayes">3.5.1.2 朴素贝叶斯的贝叶斯算法(Bayesian naive Bayes)</a></li>
          </ul>
        </li>
        <li><a href="#252-使用这个模型来预测">2.5.2 使用这个模型来预测</a></li>
        <li><a href="#353-log-sum-exp技巧">3.5.3 log-sum-exp技巧</a></li>
        <li><a href="#354-用互信息进行特征选择">3.5.4 用互信息进行特征选择</a></li>
        <li><a href="#355-使用词袋的文档分类器">3.5.5 使用词袋的文档分类器</a></li>
      </ul>
    </li>
  </ul>
</nav>


    </aside>
  
 
      </header>

      

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.github.io/notes/docs/mlapp/03generative_models_for_discrete_data/0044/">
    下一页<br>Exercises
  </a>
  
  
  <a class="older-posts" href="https://chaoskey.github.io/notes/docs/mlapp/03generative_models_for_discrete_data/0042/">
      上一页<br>3.4 狄利克雷-多项模型
  </a>
  
</nav>

<hr>

<article class="markdown">
  <h1>
    <a href="/notes/docs/mlapp/03generative_models_for_discrete_data/0043/">3.5 朴素贝叶斯分类器</a>
  </h1>
  

<div>

  <h5>2019-07-01</h5>


<div>

  
    |
    
      <a href="/notes/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
      
    |
  

  
    |
    
      <a href="/notes/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF">贝叶斯</a>
      , 
      <a href="/notes/tags/%E5%88%86%E7%B1%BB%E5%99%A8">分类器</a>
      , 
      <a href="/notes/tags/%E6%8B%9F%E5%90%88">拟合</a>
      , 
      <a href="/notes/tags/%E9%A2%84%E6%B5%8B">预测</a>
      , 
      <a href="/notes/tags/%E6%8A%80%E5%B7%A7">技巧</a>
      , 
      <a href="/notes/tags/%E4%BA%92%E4%BF%A1%E6%81%AF">互信息</a>
      , 
      <a href="/notes/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9">特征选择</a>
      
    |
  




    <a href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/03generative_models_for_discrete_data/0043.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      评论
    </a>


</div>


</div>

<p><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/"><strong>返回本章目录</strong></a></p>
<p>在本节中，我们将讨论如何对离散值特征的向量进行分类，
  
    

    <link rel="stylesheet" href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body,
                {delimiters: [{left: '$$\n', right: '\n$$', display: true}, {left: '$$', right: '$$', display: false}, 
                              {left: '\\[', right: '\\]', display: true}, {left: '\\(', right: '\\)', display: false}]});"></script>
    
  




<span class="katex">
  \(\boldsymbol{x} \in {1,\dots,K}^D\)
</span>
，其中


<span class="katex">
  \(K\)
</span>
是每个特征的值域数， 


<span class="katex">
  \(D\)
</span>
是特征的数量。我们将使用生成方法。这要求我们指定类条件分布


<span class="katex">
  \(p(\boldsymbol{x} | y=c)\)
</span>
。最简单的方法是假设特征是<strong>条件独立的</strong>, 对给定类标签。这使我们可以将类条件密度写成一维密度的乘积:</p>



<span class="katex">
  \[
p(\boldsymbol{x} | y=c, \boldsymbol{\theta}) = \prod_{j=1}^D {p(\boldsymbol{x}_j | y=c, \theta_{jc}) } \tag{3.54}
\]
</span>

<p>此模型被称为 <strong>朴素贝叶斯分类器</strong> （NBC）。</p>
<p>该模型被称为“朴素”，因为我们不希望这些特征是独立的，即使是以类标签为条件。然而，即使朴素贝叶斯假设不正确，它也常常导致分类法运作良好（Domingos and Pazzani 1997）。其中一个原因是该模型非常简单（它只有 O（CD） 参数，对于 C个类 和 D个特征），因此它相对不受过拟合的影响。</p>
<p>类条件密度的形式依赖于每个特征的类型。我们给出一些可能性：</p>
<ul>
<li>实值特征的情况下，我们可以使用高斯分布：


<span class="katex">
  \(p(\boldsymbol{x} | y=c, \boldsymbol{\theta}) = \prod_{j=1}^D {\mathcal{N}(x_j | \mu_{jc}, \sigma_{jc}^2)}\)
</span>
 ，其中


<span class="katex">
  \(\mu_{jc}\)
</span>
是c类对象的特征


<span class="katex">
  \(j\)
</span>
的均值，而


<span class="katex">
  \(\sigma_{jc}^2\)
</span>
是对应的方差。</li>
<li>二值特征的情况下，


<span class="katex">
  \(x_j \in \{0,1\}\)
</span>
，我们可以使用贝努利分布 ：


<span class="katex">
  \(p(\boldsymbol{x} | y=c, \boldsymbol{\theta}) = \prod_{j=1}^D {{\rm Ber}(x_j | \mu_{jc})}\)
</span>
，其中 


<span class="katex">
  \(\mu_{jc}\)
</span>
是c类对象的特征


<span class="katex">
  \(j\)
</span>
发生的概率。有时称之为 <strong>多变量伯努利朴素贝叶斯模型</strong>。我们将在下面看到它的一个应用。</li>
<li>在多值分类特征的情况下，


<span class="katex">
  \(x_j \in \{1,\dots,K\}\)
</span>
，我们可以使用多项分布建模： 


<span class="katex">
  \(p(\boldsymbol{x} | y=c, \boldsymbol{\theta}) = \prod_{j=1}^D {{\rm Cat}(x_j | \boldsymbol{\mu}_{jc})}\)
</span>
，其中


<span class="katex">
  \(\boldsymbol{\mu}_{jc}\)
</span>
是c类中


<span class="katex">
  \(x_j\)
</span>
的


<span class="katex">
  \(K\)
</span>
个可能值的直方图。</li>
</ul>
<p>显然，我们可以处理其他类型的特征，或使用不同的分布假设。此外，它很容易混合和匹配不同类型的特征。</p>
<h2 id="351-模型拟合">3.5.1 模型拟合</h2>
<p>我们现在讨论如何“训练”一个朴素的贝叶斯分类器。这通常意味着计算带参数的MLE或MAP估计。但是，我们还将讨论如何计算完整的后验: 


<span class="katex">
  \(p(\boldsymbol{\theta} | \mathcal{D})\)
</span>
。</p>
<h3 id="3511-nbc的mle">3.5.1.1 NBC的MLE</h3>
<p>单个数据情况的概率表示如下(译者注: 这里进行了参数的重新编排) :</p>



<span class="katex">
  \[
p(\boldsymbol{x}_i, y_j | \boldsymbol{\theta})=p(y_i | \boldsymbol{\pi}) \prod_j {p(x_{ij} | \boldsymbol{\theta}_j)}=\prod_c {\pi_c^{\mathbb{I}(y_i=c)}} \prod_j {\prod_c {p(x_{ij} | \boldsymbol{\theta}_{jc})^{\mathbb{I}(y_i=c)}}} \tag{3.55}
\]
</span>

<p>因此，对数似然由下式给出:</p>



<span class="katex">
  \[
{\rm log} \ p(\mathcal{D} | \boldsymbol{\theta})= \sum_{c=1}^C {N_c {\rm log} \ \pi_c}  + \sum_{j=1}^D {\sum_{c=1}^C { \sum_{i:y_i=c} {{\rm log} \ p(x_{ij} | \boldsymbol{\theta}_{jc})}}} \tag{3.56}
\]
</span>

<p>我们看到，这个表达式分解成一系列的项，第一项是关于参数


<span class="katex">
  \(\boldsymbol{\pi}\)
</span>
的，而DC项是关于参数


<span class="katex">
  \(\boldsymbol{\theta}_{jc}\)
</span>
的。因此，我们可以分别优化所有这些参数。</p>
<p>依据公式3.48，关于类先验的MLE可表示如下:</p>



<span class="katex">
  \[
\hat{\pi}_c = \dfrac{N_c}{N} \tag{3.57}
\]
</span>

<p>其中


<span class="katex">
  \(N_c \overset{\Delta}{=} \sum_i {\mathbb{I}(y_i=c)}\)
</span>
 是c类的样本数。</p>
<p>对这个拟然的的MLE依赖于我们为每个特征选择分布类型。为简单起见，我们假设所有特征是二项的，于是


<span class="katex">
  \(\boldsymbol{x}_j | y=c \propto {\rm Ber}(\theta_{jc})\)
</span>
。在这种情况下，MLE变为:</p>



<span class="katex">
  \[
\hat{\theta}_{jc} = \dfrac{N_{jc}}{N} \tag{3.58}
\]
</span>

<p>可以非常简单地实现该模型拟合程序：见算法8提供的伪代码（及 Matlab代码_naiveBayesFit_ ）。该算法显然需要


<span class="katex">
  \(O(N D)\)
</span>
时间。该方法易于推广以处理混合类型的特征。这种简单性是该方法有如此广泛使用的一个原因。</p>
<p>图3.8给出了一个2个类和600个二项特征的示例，用于表示在词袋模型中是否存在单词。绘制了关于2个类的


<span class="katex">
  \(\boldsymbol{\theta}_c\)
</span>
向量的可视化。索引107处的大峰值对应于单词“subject”，它t同时以概率为1出现在两个类别中。（在第3.5.4节中，我们讨论如何“过滤”这样的无信息特征。）</p>
<p><img src="" alt="0065.jpg"></p>
<p><img src="" alt="0066.jpg"></p>
<blockquote>
<p>图3.8 类条件密度


<span class="katex">
  \(p(\boldsymbol{x}_j=1 | y=c)\)
</span>
的两个文档类，对应于“X视窗”和“MS视窗”。由_naiveBayesBowDemo_生成的图。</p>
</blockquote>
<h3 id="3512-朴素贝叶斯的贝叶斯算法bayesian-naive-bayes">3.5.1.2 朴素贝叶斯的贝叶斯算法(Bayesian naive Bayes)</h3>
<p>最大拟然的麻烦是它可能过拟合。例如，考虑在图3.8的例子：&ldquo;subject&quot;这个单词对应的特征(记作特征


<span class="katex">
  \(j\)
</span>
)总是同时出现在两个类，所以, 我们的估计总是


<span class="katex">
  \(\hat{\theta}_{jc}=1\)
</span>
。如果我们遇到一封没有这个词的新电子邮件，会发生什么？我们的算法会出错，因为我们发现对两个类都有


<span class="katex">
  \(p(y=c | \boldsymbol{x}, \hat{\boldsymbol{\theta}})=0\)
</span>
！这是第3.3.4.1节讨论的黑天鹅悖论的另一种表现形式。</p>
<p>过拟合的简单解决方案是<strong>贝叶斯算法</strong>(Bayesian)。为简单起见，我们使用一个先验因式分解 (译者注: 重新编排了参数):</p>



<span class="katex">
  \[
p(\boldsymbol{\theta})=p(\boldsymbol{\pi}) \prod_{j=1}^D {\prod_{c=1}^C{p(\theta_{jc})}} \tag{3.59}
\]
</span>

<p>对


<span class="katex">
  \(\boldsymbol{\pi}\)
</span>
我们使用


<span class="katex">
  \({\rm Bir}(\boldsymbol{\alpha})\)
</span>
先验, 对每个


<span class="katex">
  \(\theta_{jc}\)
</span>
使用


<span class="katex">
  \({\rm Beta}(\beta_0,\beta_1)\)
</span>
先验。如果我们只取


<span class="katex">
  \(\boldsymbol{\alpha}=1\)
</span>
和


<span class="katex">
  \(\boldsymbol{\beta}=1\)
</span>
，对应于加1或拉普拉斯平滑。</p>
<p>结合公式3.56中的拟然因式分解和上面先验的因式分解, 可以得到下面的后验因式分解：</p>



<span class="katex">
  \[
\begin{aligned}
p(\boldsymbol{\theta} | \mathcal{D}) =& p(\boldsymbol{\pi} | \mathcal{D}) \prod_{j=1}^D {\prod_{c=1}^C{p(\theta_{jc} | \mathcal{D})}}  \\
p(\boldsymbol{\pi} | \mathcal{D}) =& {\rm Dir}(N_1+\alpha_1,\dots,N_C+\alpha_C)   \\
p(\theta_{jc} | \mathcal{D}) =& {\rm Beta}((N_c - N_{jc})+\beta_0,N_{jc}+\beta_1) 
\end{aligned} \tag{3.60-62}
\]
</span>

<p>换句话说，为了计算后验，我们只是把似然的经验计数加上先验计数即可。通过修改算法8来处理此版本的模型“拟合”是很简单的。</p>
<h2 id="252-使用这个模型来预测">2.5.2 使用这个模型来预测</h2>
<p>测试时，目标是计算:</p>



<span class="katex">
  \[
p(y=c | \boldsymbol{x}, \mathcal{D}) \propto p(y=c | \mathcal{D})  \prod_{j=1}^D {p(x_j | y=c,\mathcal{D})} \tag{3.63}
\]
</span>

<p>正确的贝叶斯程序是要对未知参数积分(译者注: 原文公式有误, 此处我做了修正):</p>



<span class="katex">
  \[
\begin{aligned}
p(y=c | \boldsymbol{x}, \mathcal{D})  \propto & \left[ \int {{\rm Cat}(y=c | \boldsymbol{\pi}) p(\boldsymbol{\pi} | \mathcal{D})} d \boldsymbol{\pi} \right]  \\
\quad & \prod_{j=1}^D { \left[ \int {{\rm Ber}(x_j | y=c, \theta_{jc}) p(\theta_{jc} | \mathcal{D})}d \theta_{jc} \right]}  
\end{aligned} \tag{3.64-65}
\]
</span>

<p>幸运的是，这是很容易做到，至少如果后验是狄利克雷分布。特别是，根据公式3.51，我们知道可以通过简单地插入后验平均参数


<span class="katex">
  \(\bar{\boldsymbol{\theta}}\)
</span>
来获得后验预测密度 θ。因此</p>



<span class="katex">
  \[
\begin{aligned}
p(y=c | \boldsymbol{x}, \mathcal{D})  & \propto \bar{\pi} _c  \prod_{j=1}^D {\left(\bar{{\theta}}_{jc}\right)^{\mathbb{I}(x_j=1)} \left(1-\bar{{\theta}}_{jc}\right)^{\mathbb{I}(x_j=0)}} \\
\bar{{\theta}}_{jc}  & = \dfrac{N_{jc}+\beta_1}{N_c+\beta_0+\beta_1}   \\
\bar{{\pi}}_c & = \dfrac{N_c+\alpha_c}{N+\alpha_0} 
\end{aligned}  \tag{3.66-68}
\]
</span>

<p>其中 ，


<span class="katex">
  \(\alpha_0=\sum_c{\alpha_c}\)
</span>
。</p>
<p>如果我们有一个单点的后验近似，


<span class="katex">
  \(p(\boldsymbol{\theta} | \mathcal{D}) \approx \delta_{\hat{\boldsymbol{\theta}}}(\boldsymbol{\theta})\)
</span>
，其中


<span class="katex">
  \(\hat{\boldsymbol{\theta}}\)
</span>
可以是MLE或MAP估计，那么 后验预测密度可通过简单地插入参数而获得，进而获得一个几乎一样的规则:</p>



<span class="katex">
  \[
p(y=c | \boldsymbol{x}, \mathcal{D})   \propto \hat{\pi} _c  \prod_{j=1}^D {\left(\hat{{\theta}}_{jc}\right)^{\mathbb{I}(x_j=1)} \left(1-\hat{{\theta}}_{jc}\right)^{\mathbb{I}(x_j=0)}} \tag{3.69}
\]
</span>

<p>唯一的区别是, 我们用后验众数或MLE


<span class="katex">
  \(\hat{\boldsymbol{\theta}}\)
</span>
来取代后验均值


<span class="katex">
  \(\bar{\boldsymbol{\theta}}\)
</span>
。然而，这种微小差异在实践中很重要，因为后验均值将导致较少的过拟合（参见第3.4.4.1节）。</p>
<h2 id="353-log-sum-exp技巧">3.5.3 log-sum-exp技巧</h2>
<p>无论使用任何类型的生成式分类器, 我们要讨论一个重要的实际细节 。尽管我们能够按公式2.13计算后验类标签， 并且使用适当的类条件密度（和近似插入）。但遗憾的是，由于<strong>数值下溢</strong>，公式2.13的简单实现可能会失败。问题出在


<span class="katex">
  \(p(\boldsymbol{x} | y=c)\)
</span>
 通常是一个非常小的数字，特别是如果


<span class="katex">
  \(\boldsymbol{x}\)
</span>
 是一个高维向量。这是因为我们要求


<span class="katex">
  \(\sum_{\boldsymbol{x}}{p(\boldsymbol{x} | y)}=1\)
</span>
，因此观察任何特定高维向量的概率很小。一个明显的解决方案是在应用贝叶斯规则时进行log，如下所示：</p>



<span class="katex">
  \[
\begin{aligned}
{\rm log} \ p(y=c | \boldsymbol{x}) = & b_c -  {\rm log} \left[\sum_{c^{'}=1}^C {e^{b_{c^{'}}}} \right] \\
b_c \overset{\Delta}{=} & {\rm log} \ p(\boldsymbol{x}|y=c) + {\rm log} \ p(y=c) 
\end{aligned} \tag{3.70-71}
\]
</span>

<p>然而，这需要计算以下这个表达式:</p>



<span class="katex">
  \[
{\rm log} \left[\sum_{c^{'}} {e^{b_{c^{'}}}} \right] = {\rm log} \sum_{c^{'}} {p(y=c^{'} | \boldsymbol{x})}={\rm log} \ p(\boldsymbol{x}) \tag{3.72}
\]
</span>

<p>我们不能在


<span class="katex">
  \({\rm log}\)
</span>
域中加减。幸运的是，我们可以将最大公因项分解出来，比如，</p>



<span class="katex">
  \[
{\rm log}  (e^{-120}+e^{-121})={\rm log}  (e^{-120}(1+e^{-1}))={\rm log}  (1+e^{-1}) - 120 \tag{3.73}
\]
</span>

<p>在一般情况下，我们有:</p>



<span class="katex">
  \[
{\rm log} \sum_c{e^{b_c}}={\rm log} \left[(\sum_c{e^{(b_c-B)}})e^B \right]= \left[{\rm log} (\sum_c{e^{(b_c-B)}}) \right] + B  \tag{3.74}
\]
</span>

<p>其中


<span class="katex">
  \(B= \underset{c}{\rm max} \ b_c\)
</span>
。这被称为 <strong>log-sum-exp</strong> 技巧，并被广泛使用。（有关请参阅函数_logsumexp_的实现。）</p>
<p>此技巧在算法1中，使用NBC来计算


<span class="katex">
  \(p(y_i | \boldsymbol{x}_i,\hat{\boldsymbol{\theta}})\)
</span>
。参阅Matlab代码 <em>naiveBayesPredict</em>。注意，如果我们只希望计算


<span class="katex">
  \(\hat{y}_i\)
</span>
, 那么我们不需要log-sum-exp技巧，因为我们只要最大化非标准量


<span class="katex">
  \({\rm log} \ p(y_i=c) + {\rm log} \ p(\boldsymbol{x}_i | y_i=c)\)
</span>
。</p>
<p><img src="" alt="0067.jpg"></p>
<h2 id="354-用互信息进行特征选择">3.5.4 用互信息进行特征选择</h2>
<p>由于NBC是拟合潜在很多特征的联合分布，因此可能会受过拟合的影响。此外，运行时开销是


<span class="katex">
  \(O(D)\)
</span>
，对于某些应用来说可能太高。</p>
<p>解决这两个问题的一种常见方法是进行<strong>特征选择</strong>，以消除对分类问题没有多大帮助的“无关”特征。最简单的特征选择方法是分别评估每个特征的相关性，然后取最高


<span class="katex">
  \(K\)
</span>
，其中


<span class="katex">
  \(K\)
</span>
的选择是基于准确性和复杂性之间的一些权衡。这种方法称为变量<strong>排序</strong>， <strong>过滤</strong>或<strong>筛选</strong>。</p>
<p>衡量相关性的一种方法是使用在特征


<span class="katex">
  \(X_j\)
</span>
和标签


<span class="katex">
  \(Y\)
</span>
之间的互信息（第2.8.3节）:</p>



<span class="katex">
  \[
I(X,Y)=\sum_{x_j} {\sum_y{p(x_j,y) \log\dfrac{p(x_j,y)}{p(x_j)p(y)}}} \tag{3.75}
\]
</span>

<p>互信息可以被认为是: 一旦我们观察到特征


<span class="katex">
  \(j\)
</span>
的值，那么标签分布上的熵就减少。如果特征是二项的，那这点很容易证明（练习3.21），这个MI可被如下计算:</p>



<span class="katex">
  \[
I_j=\sum_c{\left[\theta_{jc}\pi_c \log\dfrac{\theta_{jc}}{\theta_j} +(1-\theta_{jc}) \pi_c \log \dfrac{1-\theta_{jc}}{1-\theta_j} \right]} \tag{3.76}
\]
</span>

<p>其中


<span class="katex">
  \(\pi_c=p(y=c),\theta_{jc}=p(x_j=1|y=c), \theta_j=p(x_j=1)=\sum_c{\pi_c \theta_{jc}}\)
</span>
。（所有这些能被计算的量都是朴素贝叶斯分类器的副产品。）</p>
<p>图3.1说明了如果我们将其应用于图3.8中使用的二项词袋数据集中会发生什么。我们看到具有最高互信息的词比最有可能的词更具有辨别力。例如，两个类中最可能的单词是“subject”，它总是出现，因为这是新闻组的数据，它总是有一个主题(subject)行。但显然这不是很有辨别力的。带有类别标签的MI最高的单词是（按降序排列）“windows”，“microsoft”，“DOS”和“motif”，这是有道理的，因为这些类对应于Microsoft Windows和X Windows。</p>
<h2 id="355-使用词袋的文档分类器">3.5.5 使用词袋的文档分类器</h2>
<p><strong>文档分类器</strong> 是将文本文档分类为不同类别的问题。一种简单的方法是将每个文档表示为二项的向量，其记录每个单词是否存在与否, 于是


<span class="katex">
  \(x_{ij}=1 \quad {\rm iff} \quad\)
</span>
 单词


<span class="katex">
  \(j\)
</span>
出现在文档


<span class="katex">
  \(i\)
</span>
中, 否则


<span class="katex">
  \(x_{ij}=0\)
</span>
。然后我们可以使用如下的类条件密度：</p>



<span class="katex">
  \[
p(\boldsymbol{x}_i | y_i, \boldsymbol{\theta})=\prod_{j=1}^D {{\rm Ber}(x_{ij} | \theta_{jc})} = \prod_{j=1}^D{\theta_{jc}^{\mathbb{I}(x_{ij})} (1-\theta_{jc})^{\mathbb{I}(1-x_{ij})}} \tag{3.77}
\]
</span>

<p>这称为 <strong>伯努利乘积模型</strong>(the Bernoulli product model,)，或 <strong>二项独立模型</strong>(the binary independence model)。</p>
<p><img src="" alt="0068.jpg"></p>
<p>然而，忽略文档中每个单词出现的次数会丢失一些信息（McCallum和Nigam 1998）。更准确的表示计算每个单词的出现次数。具体来说，让


<span class="katex">
  \(\boldsymbol{x}_i\)
</span>
表示文档


<span class="katex">
  \(i\)
</span>
的计数向量，于是


<span class="katex">
  \(x_{ij} \in \{0,1,\dots,N_i\}\)
</span>
，其中


<span class="katex">
  \(N_i\)
</span>
是在文件


<span class="katex">
  \(i\)
</span>
的所有单词数（即, 


<span class="katex">
  \(\sum_{j=1}^D{x_{ij}}=N_j\)
</span>
）。对于类条件密度，我们可以使用多项分布：</p>



<span class="katex">
  \[
p(\boldsymbol{x}_i | y_i=c,\boldsymbol{\theta})={\rm Mu}(\boldsymbol{x}_i  | N_i,\boldsymbol{\theta}_c) = \dfrac{N_i!}{\prod_{j=1}^D{x_{ij}!}}\prod_{j=c}^D{\theta_{jc}^{x_{ij}}} \tag{3.78}
\]
</span>

<p>我们隐含地假设文件长度为


<span class="katex">
  \(N_i\)
</span>
独立于分类。这里


<span class="katex">
  \(\theta_{jc}\)
</span>
是


<span class="katex">
  \(c\)
</span>
类文档中生成单词


<span class="katex">
  \(j\)
</span>
的概率; 这些参数满足约束


<span class="katex">
  \(\sum_{j=1}^D{\theta_{jc}}=1, \forall \ c\)
</span>
。</p>
<p>尽管多项分类器在测试时易于训练且易于使用，但它并不是对文档分类特别有效。其中一个原因是它没有考虑到的<strong>突发性</strong>单词使用。这指的是大多数单词从未出现在任何给定文档中的现象，但如果它们确实出现一次，则它们可能不止一次出现，即单词以突发形式出现。</p>
<p>多项模型无法捕捉突发现象。要知道为什么，注意公式3.78有形如


<span class="katex">
  \(\theta_{jc}^{x_{ij}}\)
</span>
的项， 由于对于罕见词


<span class="katex">
  \(\theta_{jc} \ll 1\)
</span>
，越来越不可能产生许多词。对于更频繁的单词，衰减率不是那么快。为了直观地理解原因，请注意最常用的单词是不具体类的功能单词，例如“and”，“the”和“but”; “and”这个词出现的几率基本上是相同的，无论它先前发生了多少时间（模数长度），因此对于常用词而言，独立性假设更合理。然而，由于罕见词是最适合分类目的的词，因此我们想要最仔细地建模。</p>
<p>已经提出了各种富有启发式方法来改进文档分多项类器的性能（Rennie等人，2003）。我们现在提出了一种替代类条件密度，它与这些特殊方法一样，但概率上是合理的（Madsen等人，2005）。</p>
<p>假设我们简单地用狄利克雷混合多项(Dirichlet Compound Multinomial)密度 或 DCM 密度来代替多项类条件密度 ， 定义如下</p>



<span class="katex">
  \[
p(\boldsymbol{x}_i | y_i=c,\boldsymbol{\theta}) = \int {{\rm Mu}(\boldsymbol{x}_i|N_i,\boldsymbol{\theta}_c){\rm Dir}(\boldsymbol{\theta}_c | \boldsymbol{\alpha}_c) d \boldsymbol{\theta}_c}=\dfrac{N_i!}{\prod_{j=1}^D{x_{ij}!}}\dfrac{{\rm B}(\boldsymbol{x}_i+\boldsymbol{\alpha}_i)}{{\rm B}(\boldsymbol{\alpha}_i)} \tag{3.79}
\]
</span>

<p>（这个等式推导自公式5.24。）令人惊讶的是，这个简单的变化就是捕获突发现象所需的全部。直观的原因如下：看到一个单词的一个发生后， 比如单词


<span class="katex">
  \(j\)
</span>
，那么在


<span class="katex">
  \(\theta_j\)
</span>
上的后验计数得到更新，于是单词


<span class="katex">
  \(j\)
</span>
再次出现有更大可能。相反，如果


<span class="katex">
  \(\theta_j\)
</span>
固定，那么每个单词的出现是独立的。多项模型对应于从有K中颜色球的坛子中取出一个球，记录其颜色，然后放回。相比之下，DCM模型对应于取出一个球，记录其颜色，然后将取出的球连同一个同色额外球一起放回; 这被称为 <strong>波利亚坛子</strong>。</p>
<p>使用DCM模型作为类条件密度比使用多项模型具有更好的结果，并且具有与现有技术方法相当的性能，正如（Madsen et al. 2005）中所述。唯一的缺点是配置DCM模型更复杂; 详见了（Minka 2000e; Elkan 2006）。</p>
<p><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/"><strong>返回本章目录</strong></a></p></article>

<hr>

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.github.io/notes/docs/mlapp/03generative_models_for_discrete_data/0044/">
    下一页<br>Exercises
  </a>
  
  
  
  <a class="older-posts" href="https://chaoskey.github.io/notes/docs/mlapp/03generative_models_for_discrete_data/0042/">
      上一页<br>3.4 狄利克雷-多项模型
  </a>
  
</nav>

 

      <footer class="book-footer">
        
  <div class="flex justify-between">





  <div>
    <a class="flex align-center" href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/03generative_models_for_discrete_data/0043.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      <span>评论</span>
    </a>
  </div>
  
  
  <div>
    <a class="flex align-center" href="https://gitee.com/-/ide/project/chaoskey/notes/edit/master/-/content/docs/mlapp/03generative_models_for_discrete_data/0043.md" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        
  
  <div class="book-comments">

</div>
  
  
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#351-模型拟合">3.5.1 模型拟合</a>
          <ul>
            <li><a href="#3511-nbc的mle">3.5.1.1 NBC的MLE</a></li>
            <li><a href="#3512-朴素贝叶斯的贝叶斯算法bayesian-naive-bayes">3.5.1.2 朴素贝叶斯的贝叶斯算法(Bayesian naive Bayes)</a></li>
          </ul>
        </li>
        <li><a href="#252-使用这个模型来预测">2.5.2 使用这个模型来预测</a></li>
        <li><a href="#353-log-sum-exp技巧">3.5.3 log-sum-exp技巧</a></li>
        <li><a href="#354-用互信息进行特征选择">3.5.4 用互信息进行特征选择</a></li>
        <li><a href="#355-使用词袋的文档分类器">3.5.5 使用词袋的文档分类器</a></li>
      </ul>
    </li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  
</body>

</html>












