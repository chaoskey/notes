<!DOCTYPE html>
<html lang="cn">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="7.4 稳健线性回归*"><meta property="og:title" content="7.4 稳健线性回归*" />
<meta property="og:description" content="返回本章目录
在回归模型中，使用零均值和常数方差的高斯分布对噪声进行建模是很常见的。
  
    

    
    
    
    
    
    
    
    
  





  \(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)

 ，其中



  \(\epsilon_i=y_i-\boldsymbol{w}^T \boldsymbol{x}_i\)

。 在这种情况下，最大化拟然等价于最小化残差平方和。 但是，如果我们的数据中存在异常值，则可能导致拟合不良，如图7.6（a）所示。 （异常值是图底部的点。）这是因为平方误差以二次方处理偏差，因此远离线的点对拟合的影响大于线附近的点。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chaoskey.github.io/notes/docs/mlapp/07linear_regression/0063/" /><meta property="article:section" content="docs" />
<meta property="article:published_time" content="2019-07-21T20:20:35&#43;08:00" />
<meta property="article:modified_time" content="2019-07-21T20:20:35&#43;08:00" />

<title>7.4 稳健线性回归* | 学习笔记</title>
<link rel="icon" href="/notes/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/notes/book.min.07a0a866d76192b38577628c20f8f349c797682bd8a0d6b3d18465b8420bd2fb.css" integrity="sha256-B6CoZtdhkrOFd2KMIPjzSceXaCvYoNaz0YRluEIL0vs=">


<script defer src="/notes/cn.search.min.b24a80f802a70544ae3facb372d58456ff935b9a88f45da5f9aad2cba715f951.js" integrity="sha256-skqA&#43;AKnBUSuP6yzctWEVv&#43;TW5qI9F2l&#43;arSy6cV&#43;VE="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/notes"><img src="/notes/logo.png" alt="Logo" /><span>学习笔记</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>









  










    
    



<ul>
<li><a href="/notes/posts/"><strong>杂事记录</strong></a></li>
<li><a href="/notes/docs/fem/"><strong>有限元法自动求解微分方程</strong></a></li>
<li><a href="/notes/docs/julia/"><strong>基于Julia科学计算</strong></a></li>
<li><a href="/notes/docs/theophy/"><strong>理论物理学习笔记</strong></a></li>
<li><a href="/notes/docs/diffgeo/"><strong>微分几何笔记</strong></a></li>
<li><a href="/notes/docs/mlapp/"><strong>机器学习：概率视角</strong></a>
<ul>
<li><a href="/notes/docs/mlapp/01introduction/">第一章 导论</a></li>
<li><a href="/notes/docs/mlapp/02probability/">第二章 概率</a></li>
<li><a href="/notes/docs/mlapp/03generative_models_for_discrete_data/">第三章 基于离散数据的生成式模型</a></li>
<li><a href="/notes/docs/mlapp/04gaussian_models/">第四章 高斯模型</a></li>
<li><a href="/notes/docs/mlapp/05bayesian_statistics/">第五章 贝叶斯统计</a></li>
<li><a href="/notes/docs/mlapp/06frequentist_statistics/">第六章 频率派统计</a></li>
<li><a href="/notes/docs/mlapp/07linear_regression/">第七章 线性回归</a></li>
</ul>
</li>
<li><a href="/notes/docs/apm/"><strong>主动投资组合管理</strong></a></li>
</ul>












</nav>




  <script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>7.4 稳健线性回归*</strong>

  <label for="toc-control">
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents"></nav>


    </aside>
  
 
      </header>

      

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.github.io/notes/docs/mlapp/07linear_regression/0064/">
    下一页<br>7.5 岭回归
  </a>
  
  
  <a class="older-posts" href="https://chaoskey.github.io/notes/docs/mlapp/07linear_regression/0062/">
      上一页<br>7.3 最大似然估计（最小二乘）
  </a>
  
</nav>

<hr>

<article class="markdown">
  <h1>
    <a href="/notes/docs/mlapp/07linear_regression/0063/">7.4 稳健线性回归*</a>
  </h1>
  

<div>

  <h5>2019-07-21</h5>


<div>

  
    |
    
      <a href="/notes/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
      
    |
  

  
    |
    
      <a href="/notes/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">线性回归</a>
      , 
      <a href="/notes/tags/%E7%A8%B3%E5%81%A5%E6%80%A7">稳健性</a>
      , 
      <a href="/notes/tags/%E5%8E%9A%E5%B0%BE">厚尾</a>
      
    |
  




    <a href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/07linear_regression/0063.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      评论
    </a>


</div>


</div>

<p><a href="/notes/docs/mlapp/07linear_regression/"><strong>返回本章目录</strong></a></p>
<p>在回归模型中，使用零均值和常数方差的高斯分布对噪声进行建模是很常见的。
  
    

    <link rel="stylesheet" href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    
    
    <script defer src="https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body,
                {delimiters: [{left: '$$\n', right: '\n$$', display: true}, {left: '$$', right: '$$', display: false}, 
                              {left: '\\[', right: '\\]', display: true}, {left: '\\(', right: '\\)', display: false}]});"></script>
    
  




<span class="katex">
  \(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)
</span>
 ，其中


<span class="katex">
  \(\epsilon_i=y_i-\boldsymbol{w}^T \boldsymbol{x}_i\)
</span>
。 在这种情况下，最大化拟然等价于最小化残差平方和。 但是，如果我们的数据中存在<strong>异常值</strong>，则可能导致拟合不良，如图7.6（a）所示。 （异常值是图底部的点。）这是因为平方误差以二次方处理偏差，因此远离线的点对拟合的影响大于线附近的点。</p>
<p>实现异常值<strong>稳健性</strong>的一种方法是用<strong>厚尾</strong>分布替代高斯分布。 这样的分布将给异常值分配高拟然，而不必干扰直线来“解释”它们。</p>
<p>一种可能是使用第2.4.3节中介绍的拉普拉斯分布。 如果我们使用它作为观察模型进行回归，我们得到如下拟然：</p>



<span class="katex">
  \[
p(y|\boldsymbol{x},\boldsymbol{w},b) = {\rm Lap}(y|\boldsymbol{w}^T\boldsymbol{x},b) \propto \exp\left(-\dfrac{1}{b}|y-\boldsymbol{w}^T\boldsymbol{x}|\right)   \tag{7.24}
\]
</span>

<p>稳健性源于


<span class="katex">
  \(|y-\boldsymbol{w}^T\boldsymbol{x}|\)
</span>
的使用 而不是


<span class="katex">
  \((y-\boldsymbol{w}^T\boldsymbol{x})^2\)
</span>
。 为简单起见，我们假设b是固定的。 令


<span class="katex">
  \(r_i \overset{\Delta}{=} y_i-\boldsymbol{w}^T\boldsymbol{x}_i\)
</span>
成表示为第


<span class="katex">
  \(i\)
</span>
个残差。 于是NLL形如</p>



<span class="katex">
  \[
\ell(\boldsymbol{w}) = \sum_{i=1}^N{|r_i(\boldsymbol{w})|}   \tag{7.25}
\]
</span>

<p>不幸的是，这是一个非线性目标函数，很难优化。 幸运的是，我们可以使用以下<strong>分割变量</strong>技巧将NLL转换为受线性约束线性目标。 首先我们定义</p>



<span class="katex">
  \[
\begin{aligned}
r_i^+ \overset{\Delta}{=}& (r_i+|r_i|)/2 \ge 0 \\
r_i^- \overset{\Delta}{=}& (|r_i|-r_i)/2 \ge 0 \\
\Rightarrow \quad &\\
r_i = &r_i^+ - r_i^- ,\quad |r_i| = r_i^+ + r_i^- \\
\end{aligned} \tag{7.26}
\]
</span>

<p>于是我们绝对值目标优化问题，变成一个含不等式约束的优化问题</p>



<span class="katex">
  \[
\min_{\boldsymbol{w},\boldsymbol{r}^+,\boldsymbol{r}^-} \sum_i{r_i^+ + r_i^-} \quad s.t.\quad r_i^+ \ge 0, r_i^- \ge 0, \boldsymbol{w}^T\boldsymbol{x}_i + r_i^+ - r_i^-  = y_i  \tag{7.27}
\]
</span>

<p>这是一个有


<span class="katex">
  \(D+2N\)
</span>
个未知变量和3个约束的<strong>线性规划</strong>问题。</p>
<p>由于这是凸优化问题，因此它具有唯一的解。 要解决这个LP，我们必须先写成标准格式，如下所示：</p>



<span class="katex">
  \[
\min_{\boldsymbol{\theta}} \boldsymbol{f}^T\boldsymbol{\theta} \quad s.t. \quad \boldsymbol{A} \boldsymbol{\theta} \le \boldsymbol{b}, \boldsymbol{A}_{eq}\boldsymbol{\theta}=\boldsymbol{b}_{eq},\boldsymbol{l}\le\boldsymbol{\theta} \le \boldsymbol{u} \tag{7.28}
\]
</span>

<p>在我们当前的例子中，


<span class="katex">
  \(\boldsymbol{\theta}=[\boldsymbol{w};\boldsymbol{r}^+;\boldsymbol{r}^-]\)
</span>
，


<span class="katex">
  \(\boldsymbol{f} = [\boldsymbol{0}_{D\times1};\boldsymbol{1}_{N\times1};\boldsymbol{1}_{N\times1}]\)
</span>
，


<span class="katex">
  \(\boldsymbol{A}_{eq} = [\boldsymbol{X} ,\boldsymbol{1}_{N \times N} ,- \boldsymbol{1}_{N \times N} ]\)
</span>
，


<span class="katex">
  \(\boldsymbol{b}_{eq} = \boldsymbol{y}\)
</span>
 ，


<span class="katex">
  \(\boldsymbol{A} = []\)
</span>
，


<span class="katex">
  \(\boldsymbol{b} = []\)
</span>
，


<span class="katex">
  \(\boldsymbol{l} = [-\boldsymbol{\infty}_{D \times 1},\boldsymbol{0}_{N \times 1} ;\boldsymbol{0}_{N \times 1}]\)
</span>
，


<span class="katex">
  \(\boldsymbol{u}= []\)
</span>
。 于是可以通过任何LP求解器来解决（参见例如（Boyd和Vandenberghe 2004））。 有关实际方法的示例，请参见图7.6（a）。</p>
<p>在拉普拉斯似然下使用NLL的另一种方法是最小化<strong>Huber损失</strong>函数（Huber 1964），定义如下：</p>



<span class="katex">
  \[
L_H(r,\delta)=\left\{
\begin{aligned}
r^2/2 \quad& if \quad |r| \le \delta \\
\delta|r|-\delta^2/2 \quad& if \quad |r| > \delta \\
\end{aligned}
\right.  \tag{7.29}
\]
</span>

<p>当误差小于


<span class="katex">
  \(\delta\)
</span>
时，相当于


<span class="katex">
  \(\ell_2\)
</span>
; 当误差大于


<span class="katex">
  \(\delta\)
</span>
时，相当于


<span class="katex">
  \(\ell_1\)
</span>
。 参见图7.6（b）。 这种损失函数的优点在于处处可微，实际上


<span class="katex">
  \(\frac{d}{dr}|r|={\rm sign}(r), \ if \ r \ne 0\)
</span>
。并且此函数是


<span class="katex">
  \(C_1\)
</span>
连续的，因为函数的两部分交界处


<span class="katex">
  \(r =\pm \delta\)
</span>
的梯度是匹配的，即


<span class="katex">
  \(\left. \frac{d}{dr}L_H(r,\delta)\right|_{r=\delta}=\delta\)
</span>
。 因此，优化Huber损失比使用拉普拉斯似然快得多，因为我们可以使用标准平滑优化方法（例如quasiNewton）而不是线性规划。</p>
<p>图7.6（a）给出了Huber损失函数的图示。 结果在定性地类似于概率方法。 （事实上，Huber方法也有概率解释，尽管它很不自然（Pontil et al.1998）。）</p>
<p><img src="" alt="0098.jpg"></p>
<blockquote>
<p>图7.6 （a）稳健线性回归的图示。 由_linregRobustDemoCombined_生成的图。 （b）


<span class="katex">
  \(\ell_2\)
</span>
，


<span class="katex">
  \(\ell_1\)
</span>
和Huber损失函数的图示。 _huberLossDemo_生成的图。</p>
</blockquote>
<p><a href="/notes/docs/mlapp/07linear_regression/"><strong>返回本章目录</strong></a></p></article>

<hr>

<nav class="post-pagination">
  
  <a class="newer-posts" href="https://chaoskey.github.io/notes/docs/mlapp/07linear_regression/0064/">
    下一页<br>7.5 岭回归
  </a>
  
  
  
  <a class="older-posts" href="https://chaoskey.github.io/notes/docs/mlapp/07linear_regression/0062/">
      上一页<br>7.3 最大似然估计（最小二乘）
  </a>
  
</nav>

 

      <footer class="book-footer">
        
  <div class="flex justify-between">





  <div>
    <a class="flex align-center" href="https://gitee.com/chaoskey/notes/blob/master/content/docs/mlapp/07linear_regression/0063.md#blob-comment" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Comment" />
      <span>评论</span>
    </a>
  </div>
  
  
  <div>
    <a class="flex align-center" href="https://gitee.com/-/ide/project/chaoskey/notes/edit/master/-/content/docs/mlapp/07linear_regression/0063.md" target="_blank" rel="noopener">
      <img src="/notes/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        
  
  <div class="book-comments">

</div>
  
  
      </footer>
      
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents"></nav>

 
    </aside>
    
  </main>

  
</body>

</html>












