---
title: "2.2 概率论的简要回顾"
date: 2019-06-21T20:20:35+08:00
draft: true
categories: ["机器学习"]
tags: []
---

# 2.2 概率论的简要回顾

[**返回本章目录**]({{< relref "/docs/mlapp/02probability" >}})

本节是对概率论基础知识的简要回顾，仅仅是对可能“荒疏”的读者的复习。 已经熟悉这些基础知识的读者可以安全地跳过本节。

## 2.2.1 离散随机变量

表达式$$p(A)$$表示事件A为真的概率。 例如，A可能是逻辑表达“明天会下雨”。 我们要求$$0 \le p(A) \le 1$$，其中$$p(A)= 0$$表示事件肯定不会发生，而$$p(A)= 1$$表示事件肯定会发生。 我们写$$p(\bar{A})$$来表示事件不是A的概率; 这被定义为$$p(\bar{A})= 1-p(A)$$。 我们经常写$$A = 1$$表示事件A为真，$$A = 0$$表示事件A为假。

我们可以通过定义一个**离散的随机变量**$$X$$来扩展二元事件的概念，它可以从有限或可数无限集$$\mathcal{X}$$中获取任何值。 我们将事件$$X = x$$的概率表示为$$p(X = x)$$，或简写为$$p(x)$$。 这里$$p()$$被称为**概率质量函数**\(**probability mass function**\)或**pmf**。 它满足$$0 \le p(x) \le 1$$和$$\sum_{x \in \mathcal{X}}{p(x)}= 1$$的性质。 图2.1显示了在有限**状态空间**$$\mathcal{X} = \{1,2,3,4,5\}$$上定义的两个pmf。 在左边我们有一个均匀分布$$p(x)= 1/5$$，在右边我们有一个退化分布\(degenerate distribution\)$$p(x)=\mathbb{I}(x = 1)$$，其中$$\mathbb{I}()$$是二元**指示函数**。 该分布表示X始终等于值1的事实，换句话说，它是常数。

![&#x56FE;2.1](../../images/0042.jpg)

> 图2.1 （A）{1,2,3,4}上的均匀分布，$$p(x = k)= 1/4$$。 （b）如果x = 1，则退化分布$$p(x)= 1$$，如果$$x \in \{2,3,4\}$$，则$$p(x)= 0$$。 由_discreteProbDistFig_生成的图

## 2.2.2 基本规则

在本节中，我们将回顾概率的基本规则。

### 2.2.2.1 两个事件并的概率

给定两个事件A和B，我们将**A或B**的概率定义如下：

$$
\begin{aligned}
p(A \lor B)=& p(A)+p(B)-p(A \land B)  \\
\quad = & P(A)+p(B) \quad 如果A和B是互斥的   \\
\end{aligned} \tag{2.1-2}
$$

### 2.2.2.2 联合概率

我们将事件A和B的联合概率定义如下：

$$
P(A,B)=P(A \ \text{and} \ B)=p(A|B)p(B)   \tag{2.3}
$$

这有时被称为**乘法规则**。 给定对两个事件的联合分布$$p(A，B)$$，那么可将**边际分布**定义如下：

$$
p(A)=\sum_b{p(A,B)}=\sum_b{p(A|B=b)p(B=b)}   \tag{2.4}
$$

其中, 我们对B的所有可能状态进行了求和。可类似地定义$$p(B)$$。 有时被称为**求和规则**或**全概率规则**\(**rule of total probability**\)。

可以多次应用乘法规则就产生了**概率链式规则**：

$$
p(X_{1:D})=p(X_1)p(X_2|X_1)p(X_3|X_2,X_1)p(X_4|X_3,X_2,X_1) \dots p(X_D|X_{1:D-1})   \tag{2.5}
$$

我们在这里引入类似Matlab的符号$$1:D$$来表示集合$$\{1,2,\dots,D\}$$。

### 2.2.2.3  条件概率

在事件B为真的情况下，我们定义事件A的条件概率，如下所示：

$$
p(A|B)=\dfrac{p(A,B)}{p(B)} \quad if p(B)>0   \tag{2.6}
$$

## 2.2.3 贝叶斯规则

将条件概率的定义与乘积和求和规则相结合，得到**贝叶斯规则**，也称为**贝叶斯定理**:

$$
p(X=x|Y=y)=\dfrac{p(X=x,Y=y)}{p(Y=y)}=\dfrac{p(X=x)p(Y=y|X=x)}{\sum_{x'}{p(X=x')p(Y=y|X=x')}}\tag{2.7}
$$

### 2.2.3.1 例：医学诊断

作为如何使用此规则的示例，请考虑以下医学诊断问题。 假设你是一个40多岁的女性，你决定进行乳房癌的医学测试，称为**乳房X光检查**。 如果检测结果为阳性，您患癌症的概率是多少？ 这显然取决于测试的可靠性。 假设您被告知测试的**灵敏度**为80％，这意味着，如果您患有癌症，则测试为阳的概率为0.8。 换一种说法

$$
p(x=1|y=1)=0.8   \tag{2.8}
$$

其中$$x = 1$$是乳房X线检查为阳性的事件，$$y = 1$$是乳腺癌的事件。 许多人认为他们80％的可能患有癌症。 但这是假的！ 它忽略了患乳腺癌的先验概率，幸运的是它很低：

$$
p(y=1)=0.004   \tag{2.9}
$$

忽略这个先验被称为**基本比率谬误**\(**base rate fallacy**\)。 我们还需要考虑到测试可能是**假阳**或**误报**的事实。 不幸的是，这种假阳是很可能的（使用目前的筛选技术）：

$$
p(x=1|y=0)=0.1   \tag{2.10}
$$

使用贝叶斯规则组合上述三式，我们可以如下计算正确的答案：

$$
\begin{aligned}
p(y=1|x=1)=&\dfrac{p(x=1|y=1)p(y=1)}{p(x=1|y=1)p(y=1)+p(x=1|y=0)p(y=0)}    \\
\quad = &\dfrac{0.8 \times 0.004}{0.8 \times 0.004+0.1 \times 0.996}=0.031 \\
\end{aligned}  \tag{2.11-12}
$$

其中$$p(y=0)= 1 -p(y=1)= 0.996$$。 换句话说，如果你测试为阳性，你实际患乳腺癌的几率只有3％！

### 2.2.3.2 示例：生成式分类器

我们可以推广医学诊断的例子来对任意类型的特征向量$$\boldsymbol{x}$$进行如下分类：

$$
p(y=c|\boldsymbol{x},\boldsymbol{\theta})=\dfrac{p(y=c|\boldsymbol{\theta})p(\boldsymbol{x}|y=c,\boldsymbol{\theta})}{\sum_{c'}p(y=c'|\boldsymbol{\theta})p(\boldsymbol{x}|y=c',\boldsymbol{\theta})}    \tag{2.13}
$$

这被称为**生成式分类器**\(**generative classifier**\)，因为它指定了如何使用**类条件密度**$$p(\boldsymbol{x}|y=c)$$和**类先验**$$p(y=c)$$来生成数据。 我们将在第3章和第4章中详细讨论这些模型。另一种方法是直接拟合**类后验**$$p(y=c|\boldsymbol{x})$$; 这被称为**判别式分类器**。 我们将在8.6节讨论这两种方法的优缺点。

## 2.2.4 独立性和条件独立性

我们说$$X$$和$$Y$$是**无条件独立的**或**边缘独立**的\(记作为$$X \bot Y$$\)，如果我们可以将它们的联合分布表示为两个边缘分布的乘积（见图2.2），即

$$
X \bot Y \Leftrightarrow p(X,Y)=p(X)p(Y)    \tag{2.14}
$$

![&#x56FE;2.2](../../images/0043.jpg)

> 图2.2 计算$$p(X,Y)= p(X)p(Y)$$，其中$$X \bot Y$$。 这里X和Y是离散随机变量; X有6种可能的状态（值），Y有5种可能的状态。 对两个这样的变量的一般联合分布将需要（6×5） - 1 = 29个参数来定义它（我们减去1，因为有一个求和到1的约束）。 通过假设（无条件）独立性，我们只需要（6-1）+（5-1）= 9个参数来定义$$p(X,Y)$$。

一般来说，如果一组变量的联合分布可以写成边缘分布的乘积，我们说这组变量是**相互独立的**。

不幸的是，无条件独立很少见，因为大多数变量都会影响大多数其他变量。 然而，通常这种影响是通过其他变量调节的，而不是直接的。 因此，我们说$$X$$和$$Y$$对给定$$Z$$是**条件独立的**（**CI**）, 当且仅当 , 如果条件联合可以写成条件边际的乘积：

$$
X \bot Y | Z \Leftrightarrow p(X,Y|Z)=p(X|Z)p(Y|Z)   \tag{2.15}
$$

当我们在第10章讨论图模型时，我们将看到我们可以将这个假设写成图$$X -Z -Y$$，它捕获了$$X$$和$$Y$$之间的所有依赖关系都是通过Z调解的直觉。例如，已经知道今天是否下雨（事件Z）的条件下，它明天下雨（事件X）的概率与今天地面是否潮湿（事件Y）无关。 直觉上，这是因为Z“导致”X和Y，所以如果我们知道Z，我们不需要知道Y以便预测X，反之亦然。 我们将在第10章中扩展这个概念。

CI的另一个性质是：

**定理2.2.1.** $$X \bot Y | Z$$，当且仅当，存在函数$$g$$和$$h$$，满足

$$
p(x,y|z)=g(x,z)h(y,z),\quad p(z)>0 \quad \forall x,y,z  \tag{2.16}
$$

有关证明，请参阅习题2.8。

CI假设允许我们从小块构建大型概率模型。 我们将在本书中看到许多这方面的例子。 特别是，在第3.5节中，我们讨论了朴素贝叶斯分类器，在第17.2节中，我们讨论了马尔可夫模型，在第10章中我们讨论了图模型; 所有这些模型都大量利用CI属性。

## 2.2.5 连续随机变量

到目前为止，我们只考虑了不确定离散量。 我们现在将展示（以下（Jaynes 2003，p107））如何将概率扩展到不确定连续量。

假设$$X$$是一个不确定的连续数量。 $$X$$位于任何区间$$a \le X \le b$$的概率可以如下计算。 定义事件$$A =(X \le a)$$，$$B =(X \le b)$$ 和$$W =(a < X \le b)$$。我们有$$B =A \vee W$$，并且因为$$A$$和$$W$$是互斥的，所以求和规则给出

$$
p(B)=p(A)+p(W)   \tag{2.17}
$$

进而

$$
p(W)=p(B)-p(A)    \tag{2.18}
$$

定义函数$$F(q) \overset{\Delta}{=} p(X \le q)$$ 。 这称为**累积分布函数**或$$X$$的**cdf**。这显然是单调递增函数。 有关示例，请参见图2.3（a）。 使用这种表示法我们有

$$
p(a \lt X \le b)=F(b)-F(a)   \tag{2.19}
$$

现在定义$$f(x)=\frac{d}{dx} F(x)$$（我们假设这个导数存在）; 这称为**概率密度函数**或**pdf**。 有关示例，请参见图2.3（b）。 给定pdf，我们可以计算连续变量在有限区间内的概率，如下所示：

$$
P(a \lt X \le b)=\int_a^b{f(x)dx}   \tag{2.20}
$$

在很小的区间，我们可以写

$$
P(x \le X \le x+dx)\approx p(x)dx   \tag{2.21}
$$

我们要求$$p(x) \ge 0$$，但对于任何给定的$$x$$，$$p(x)> 1$$都是可能的，只要密度可积分到1.例如，考虑**均匀分布**$${\rm Unif}(a,b)$$：

$$
{\rm Unif}(a,b)=\dfrac{1}{a+b}\mathbb{I}(a \le x \le b)   \tag{2.22}
$$

如果我们设$$a = 0$$且$$b = \frac{1}{2}$$ ，则对于任何$$x \in [0,\frac{1}{2}]$$，我们有$$p(x)= 2$$。

![&#x56FE;2.3](../../images/0044.jpg)

> 图2.3 （a）标准正态$$\mathcal{N}(0,1)$$的cdf图。 （b）相应的pdf。 阴影区域各自包含概率质量的α/ 2。 因此，非阴影区域包含概率质量的1 - α。 如果分布是高斯$$\mathcal{N}(0,1)$$，则最左边的截止点是$$\Phi^{-1}(\alpha/2)$$，其中$$\Phi$$是高斯的cdf。 通过对称性，最右边的截止点是$$\Phi^{-1}(1-\alpha/2)=-\Phi^{-1}(\alpha/2)$$。 如果α= 0.05，则中心间隔为95％，左截止值为-1.96，右边为1.96。 由_quantileDemo_生成的图。

## 2.2.6 分位数

由于cdf $$F$$是单调递增函数，因此它具有逆函数; 让我们用$$F ^{-1}$$来表示这一点。 如果$$F$$是$$X$$的cdf，则$$F^{-1}(\alpha)$$是$$x_\alpha$$的值，满足$$P(X \le x_\alpha)=\alpha$$; 那么成称$$x_\alpha$$为$$F$$的$$\alpha$$**分位数**。值$$F^{-1}(0.5)$$则是分布的**中位数**，左边是概率质量的一半，右边是另一半。 值$$F^{-1}(0.25)$$和$$F^{-1}(0.75)$$分别是**下四分位数**和**上四分位数**。

我们还可以使用逆cdf来计算**尾区概率**。 例如，如果$$\Phi$$是高斯分布$$\mathcal{N}(0,1)$$的cdf，则$$\Phi^{-1}(\alpha/ 2)$$左侧的点包含α/ 2的概率质量，如图2.3（b）所示。 通过对称性，$$\Phi^{-1}(1-\alpha/2)$$ 右侧的点也包含质量的α/ 2。 因此，中心区间$$(\Phi^{-1}(\alpha/2),\Phi^{-1}(1-\alpha/2))$$包含质量的1-α。 如果我们设置α= 0.05，则中心95％区间被范围覆盖

$$
(\Phi^{-1}(0.025),\Phi^{-1}(0.975))=(-1.96,1.96)   \tag{2.23}
$$

如果分布为$$\mathcal{N}(\mu,\sigma^2)$$，则95％区间变为（μ-1.96σ，μ+1.96σ）。 有时这种近似可写成μ±2σ。

## 2.2.7 均值和方差

分布最熟悉的属性是其**均值**或**期望值**，用$$\mu$$表示。 对于离散随机变量，它被定义为$$\mathbb{E} [X] \overset{\Delta}{=} \sum_{x \in \mathcal{X}}{x p(x)}$$，对于连续随机变量，它被定义为$$\mathbb{E} [X] \overset{\Delta}{=} \int_{\mathcal{X}}{x p(x) dx}$$。 如果这个积分不是有限的，则均值没有定义（我们稍后会看到一些这样的例子）。

**方差**是分布“扩散”的度量，用$$\sigma^2$$表示。 这定义如下：

$$
\begin{aligned}
{\rm var}[X] \overset{\Delta}{=} & \mathbb{E}\left[(X-\mu)^2\right]=\int{(x-\mu)^2p(x)dx}  \\
\quad = & \int{x^2p(x)dx}+ \mu^2 \int{p(x)dx} - 2 \mu \int{ x p(x)dx}=\mathbb{E}[X^2]-\mu^2  \\
\end{aligned} \tag{2.24-25}
$$

从中我们得出有用的结果

$$
\mathbb{E}[X^2]=\sigma^2+\mu^2  \tag{2.26}
$$

**标准偏差**\(简称**标准差**\)定义为

$$
{\rm std}[X] \overset{\Delta}{=}\sqrt{{\rm var}(X)}  \tag{2.27}
$$

这很有用，因为它与$$X$$本身具有相同的单位。

[**返回本章目录**]({{< relref "/docs/mlapp/02probability" >}})

