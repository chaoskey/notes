---
title: "7.4 稳健线性回归*"
date: 2019-07-21T20:20:35+08:00
draft: true
categories: ["机器学习"]
tags: ["线性回归", "稳健性", "厚尾"]
---


[**返回本章目录**]({{< relref "/docs/mlapp/07linear_regression" >}})

在回归模型中，使用零均值和常数方差的高斯分布对噪声进行建模是很常见的。$\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ ，其中$\epsilon_i=y_i-\boldsymbol{w}^T \boldsymbol{x}_i$。 在这种情况下，最大化拟然等价于最小化残差平方和。 但是，如果我们的数据中存在**异常值**，则可能导致拟合不良，如图7.6（a）所示。 （异常值是图底部的点。）这是因为平方误差以二次方处理偏差，因此远离线的点对拟合的影响大于线附近的点。

<!--more-->

实现异常值**稳健性**的一种方法是用**厚尾**分布替代高斯分布。 这样的分布将给异常值分配高拟然，而不必干扰直线来“解释”它们。

一种可能是使用第2.4.3节中介绍的拉普拉斯分布。 如果我们使用它作为观察模型进行回归，我们得到如下拟然：

$$
p(y|\boldsymbol{x},\boldsymbol{w},b) = {\rm Lap}(y|\boldsymbol{w}^T\boldsymbol{x},b) \propto \exp\left(-\dfrac{1}{b}|y-\boldsymbol{w}^T\boldsymbol{x}|\right)   \tag{7.24}
$$

稳健性源于$|y-\boldsymbol{w}^T\boldsymbol{x}|$的使用 而不是$(y-\boldsymbol{w}^T\boldsymbol{x})^2$。 为简单起见，我们假设b是固定的。 令$r_i \overset{\Delta}{=} y_i-\boldsymbol{w}^T\boldsymbol{x}_i$成表示为第$i$个残差。 于是NLL形如

$$
\ell(\boldsymbol{w}) = \sum_{i=1}^N{|r_i(\boldsymbol{w})|}   \tag{7.25}
$$

不幸的是，这是一个非线性目标函数，很难优化。 幸运的是，我们可以使用以下**分割变量**技巧将NLL转换为受线性约束线性目标。 首先我们定义

$$
\begin{aligned}
r_i^+ \overset{\Delta}{=}& (r_i+|r_i|)/2 \ge 0 \\
r_i^- \overset{\Delta}{=}& (|r_i|-r_i)/2 \ge 0 \\
\Rightarrow \quad &\\
r_i = &r_i^+ - r_i^- ,\quad |r_i| = r_i^+ + r_i^- \\
\end{aligned} \tag{7.26}
$$

于是我们绝对值目标优化问题，变成一个含不等式约束的优化问题

$$
\min_{\boldsymbol{w},\boldsymbol{r}^+,\boldsymbol{r}^-} \sum_i{r_i^+ + r_i^-} \quad s.t.\quad r_i^+ \ge 0, r_i^- \ge 0, \boldsymbol{w}^T\boldsymbol{x}_i + r_i^+ - r_i^-  = y_i  \tag{7.27}
$$

这是一个有$D+2N$个未知变量和3个约束的**线性规划**问题。

由于这是凸优化问题，因此它具有唯一的解。 要解决这个LP，我们必须先写成标准格式，如下所示：

$$
\min_{\boldsymbol{\theta}} \boldsymbol{f}^T\boldsymbol{\theta} \quad s.t. \quad \boldsymbol{A} \boldsymbol{\theta} \le \boldsymbol{b}, \boldsymbol{A}_{eq}\boldsymbol{\theta}=\boldsymbol{b}_{eq},\boldsymbol{l}\le\boldsymbol{\theta} \le \boldsymbol{u} \tag{7.28}
$$

在我们当前的例子中，$\boldsymbol{\theta}=[\boldsymbol{w};\boldsymbol{r}^+;\boldsymbol{r}^-]$，$\boldsymbol{f} = [\boldsymbol{0}_{D\times1};\boldsymbol{1}_{N\times1};\boldsymbol{1}_{N\times1}]$，$\boldsymbol{A}_{eq} = [\boldsymbol{X} ,\boldsymbol{1}_{N \times N} ,- \boldsymbol{1}_{N \times N} ]$，$\boldsymbol{b}_{eq} = \boldsymbol{y}$ ，$\boldsymbol{A} = []$，$\boldsymbol{b} = []$，$\boldsymbol{l} = [-\boldsymbol{\infty}_{D \times 1},\boldsymbol{0}_{N \times 1} ;\boldsymbol{0}_{N \times 1}]$，$\boldsymbol{u}= []$。 于是可以通过任何LP求解器来解决（参见例如（Boyd和Vandenberghe 2004））。 有关实际方法的示例，请参见图7.6（a）。

在拉普拉斯似然下使用NLL的另一种方法是最小化**Huber损失**函数（Huber 1964），定义如下：

$$
L_H(r,\delta)=\left\{
\begin{aligned}
r^2/2 \quad& if \quad |r| \le \delta \\
\delta|r|-\delta^2/2 \quad& if \quad |r| > \delta \\
\end{aligned}
\right.  \tag{7.29}
$$

当误差小于$\delta$时，相当于$\ell_2$; 当误差大于$\delta$时，相当于$\ell_1$。 参见图7.6（b）。 这种损失函数的优点在于处处可微，实际上$\frac{d}{dr}|r|={\rm sign}(r), \ if \ r \ne 0$。并且此函数是$C_1$连续的，因为函数的两部分交界处$r =\pm \delta$的梯度是匹配的，即$\left. \frac{d}{dr}L_H(r,\delta)\right|_{r=\delta}=\delta$。 因此，优化Huber损失比使用拉普拉斯似然快得多，因为我们可以使用标准平滑优化方法（例如quasiNewton）而不是线性规划。

图7.6（a）给出了Huber损失函数的图示。 结果在定性地类似于概率方法。 （事实上，Huber方法也有概率解释，尽管它很不自然（Pontil et al.1998）。）

![[0098.jpg]]

> 图7.6 （a）稳健线性回归的图示。 由_linregRobustDemoCombined_生成的图。 （b）$\ell_2$，$\ell_1$和Huber损失函数的图示。 _huberLossDemo_生成的图。

[**返回本章目录**]({{< relref "/docs/mlapp/07linear_regression" >}})

