---
title: "6.4 估计器的理想属性"
date: 2019-07-15T20:20:35+08:00
draft: true
categories: ["机器学习"]
tags: []
---

# 6.4 估计器的理想属性

[**返回本章目录**]({{< relref "/docs/mlapp/06frequentist_statistics" >}})

由于频率派决策理论没有提供选择最佳估计器的自动方法，我们需要提出其他启发式方法来选择它们。 在本节中，我们将讨论我们所希望估计器应该具有的一些属性。 不幸的是，我们将看到我们无法同时实现所有这些属性。

## 6.4.1 一致估计器\(Consistent estimators\)

如果随着样本大小趋于无穷大而最终恢复生成数据的真实参数，即$$| \mathcal{D}|\to \infty \Rightarrow \hat{\theta}(\mathcal{D}) \to \theta^{\\*}$$ （单线箭头表示概率收敛），那么该估计器被称为**一致的**。 当然，这个概念只有在数据实际来自具有参数$$\theta^{\\*}$$的指定模型时才有意义，而实际数据通常不是这种情况。 然而，它可能是一个有用的理论属性。

可以证明MLE是一致估计器。 直观的原因是最大似然相当于最小化$$\mathbb{KL}(p(\cdot|\boldsymbol{\theta}^{\\*})| p(\cdot|\hat{\boldsymbol{\theta}}))$$，其中$$p(\cdot|\boldsymbol{\theta}^{\\*})$$是真实分布，$$p(\cdot|\hat{\boldsymbol{\theta}})$$是我们的估计。 我们可以实现0KL散度，当且仅当$$\hat{\boldsymbol{\theta}}=\boldsymbol{\theta}^{\\*}$$。

## 6.4.2 无偏估计器\(Unbiased estimators\)

估计器的**偏差**\(**bias**\)定义为

$$
{\rm bias}(\hat{\theta}(\cdot))=\mathbb{E}_{p(\mathcal{D}|\theta_{\\*})}\left[\hat{\theta}(\mathcal{D})-\theta_{\\*}\right] \tag{6.32}
$$

其中$$\theta_{\\*}$$ 是真实参数值。 如果偏差为零，则估计器被称为**无偏的**\(**unbiased**\)。 这意味着采样分布以真实参数为中心。 例如，高斯均值的MLE是无偏的：

$$
{\rm bias}(\hat{\mu})=\mathbb{E}\left[\bar{x}\right]-\mu=\mathbb{E}\left[\dfrac{1}{N}\sum_{i=1}^N{x_i}\right]-\mu=\dfrac{N \mu}{N}-\mu=0 \tag{6.33}
$$

然而，高斯方差的MLE$$\hat{\sigma}^2$$不是$$\sigma^2$$的无偏估计。 事实上，可以证明（练习6.3）

$$
\mathbb{E}\left[\hat{\sigma}^2\right]=\dfrac{N-1}{N}\sigma^2 \tag{6.34}
$$

但是，下面这个估计器

$$
\hat{\sigma}_{N-1}^2=\dfrac{N}{N-1}\hat{\sigma}^2=\dfrac{1}{N-1}\sum_{i=1}^N{(x_i-\bar{x})^2}  \tag{6.35}
$$

是无偏估计器，我们很容易证明如下:

$$
\mathbb{E}\left[\hat{\sigma}_{N-1}^2\right]=\mathbb{E}\left[\dfrac{N}{N-1}\hat{\sigma}^2\right]=\dfrac{N}{N-1}\dfrac{N-1}{N}\sigma^2=\sigma^2  \tag{6.36}
$$

在Matlab中，var\(X\)返回$$\hat{\sigma}_{N-1}^2$$，而var\(X,1\)返回$$\sigma^2$$（MLE）。 对于足够大的N，差异可以忽略不计。

尽管MLE有时可能是一个有偏差的估计器，但人们可以渐近地认为它总是无偏见的。 （这对于MLE是一致估计器来说是必要的。）

虽然无偏听起来像一个理想的属性，但并非总是如此。 有关这一点的讨论，请参见第6.4.4节和（Lindley 1972）。

## 6.4.3 最小方差估计器\(Minimum variance estimators\)

我们希望我们的估算器是无偏的，似乎在直觉上是合理的（尽管我们将在下面提出一些反对这一主张的论据）。 但是，无偏是不够的。 例如，假设我们想要从$$\mathcal{D} = \{x_1,\dots,x_N\}$$估计高斯均值。 仅查看第一个数据点$$\hat{\theta}(\mathcal{D})= x_1$$的估计器是无偏估计器，但通常比经验均值$$\bar{x}$$（也是无偏的）更远离$$\theta_{\\*}$$。 因此估计器的方差也很重要。

一个自然的问题是：方差可以持续多久？\(how long can the variance go?\) 一个著名的结果，称为**Cramer-Rao下界**，提供了无偏估计器的方差下界。 更确切地说，

**定理6.4.1.**（Cramer-Rao不等式） 令$$X_1,\dots,X_n \sim p(X |\theta_0)$$, 并且$$\hat{\theta}=\hat{\theta}(x_1,\dots,x_n)$$是$$\theta_0$$的无偏估计器。 那么，在$$p(X |\theta_0)$$的各种平滑假设下，我们有

$$
{\rm var}\left[\hat{\theta}\right] \ge \dfrac{1}{n I(\theta_0)}  \tag{6.37}
$$

其中$$I(\theta_0)$$是Fisher信息矩阵 \(参见第6.2.2节\)。

证明可以在比如（Rice 1995，p275）中找到。

可以证明MLE达到了CramerRao下界，因此具有任何无偏估计器的最小渐近方差。 因此，MLE被认为是**渐近最优的**。

## 6.4.4 偏差-方差权衡

虽然使用无偏估计器似乎是一个好主意，但情况并非总是如此。 为了解原因，假设我们使用二次损失。 如上所示，相应的风险是MSE。 我们现在推导一个非常有用的MSE分解。 （所有的期望和方差都是关于真正的分布$$p(\mathcal{D} |\theta^{\\*})$$的，但是为了符号简洁我们放弃了显式条件。）设$$\hat{\theta}=\hat{\theta}(\mathcal{D})$$表示估计，$$\bar{\theta}=\mathbb{E}\left[\hat{\theta}\right]$$表示估计的期望值。 估计（会随$$\mathcal{D}$$而变）。 于是我们有

$$
\begin{aligned}
\mathbb{E}\left[(\hat{\theta}-\theta^{*})^2\right]=& \mathbb{E}\left[\left[(\hat{\theta}-\bar{\theta})+(\bar{\theta}-\theta^{*})\right]^2\right]  \\
\quad =& \mathbb{E}\left[(\hat{\theta}-\bar{\theta})^2\right]+2(\bar{\theta}-\theta^{*})\mathbb{E}\left[(\hat{\theta}-\bar{\theta})\right]+(\bar{\theta}-\theta^{*})^2   \\
\quad =& \mathbb{E}\left[(\hat{\theta}-\bar{\theta})^2\right]+(\bar{\theta}-\theta^{*})^2  \\
\quad =& {\rm var}\left[\hat{\theta}\right]+{\rm bias}^2(\hat{\theta})  
\end{aligned} \tag{6.38-41}
$$

用文字表述为:

$$
\boxed{{\rm MSE}={\rm variance}+{\rm bias}^2} \tag{6.42}
$$

这称为**偏差-方差权衡**（参见例如（Geman等人，1992））。 这意味着使用偏差估计器可能是明智的，只要它减少我们的方差，进而假设我们的目标是最小化平方误差。

### 6.4.4.1 示例：估计高斯均值

让我们举一个例子，基于（Hoff 2009，p79）。 假如我们想要从$$\boldsymbol{x} =(x_1,\dots,x_N)$$估计高斯均值。 我们假设数据是从$$x_i \sim \mathcal{N}(\theta^{\\*} = 1，σ2)$$中采样的。 一个明显的估计是MLE。 它的偏差为0，方差为

$$
{\rm var}[\bar{x}|\theta^{\\*}]=\dfrac{\sigma^2}{N}  \tag{6.43}
$$

但我们也可以使用MAP估计。 在4.6.1节中，我们证明了在$$\mathcal{N}(\theta_0,\sigma^2/\kappa_0)$$形式的高斯先验下的MAP估计由下式给出：

$$
\tilde{x}\overset{\Delta}{=}\dfrac{N}{N+\kappa_0}\bar{x}+\dfrac{\kappa_0}{N+\kappa_0}\theta_0=w \bar{x}+(1-w)\theta_0  \tag{6.44}
$$

其中$$0 \le w \le 1$$控制我们相信MLE与先验相比的程度。 （这也是后验均值，因为高斯的均值和众数是相同的。）偏差和方差由下式给出：

$$
\begin{aligned}
\mathbb{E}[\tilde{x}]-\theta^{*}=&w \theta^{*}+(1-w)
\theta_0-\theta^{*}=(1-w)(\theta_0-\theta^{*})    \\
{\rm var}[\tilde{x}]=& w^2\dfrac{\sigma^2}{N} 
\end{aligned} \tag{6.45-46}
$$

因此，虽然MAP估计是有偏的（假设w &lt;1），但它具有较低的方差。

![&#x56FE;6.4](../../images/0089.jpg)

> 图6.4 左：采用不同先验强度$$\kappa_0$$的MAP估计的采样分布。 （MLE对应于$$\kappa_0=0$$.）右：相对于不同样本大小的MLE的MSE。 基于图5.6（Hoff 2009）。 由_samplingDistGaussShrinkage_生成的图。

![&#x56FE;6.5](../../images/0090.jpg)

> 图6.5 岭回归的偏差-方差权衡的描述。 我们从真实函数生成100个数据集，以纯绿色显示。 左图：我们绘制了20个不同数据集的正则拟合。 我们使用具有高斯RBF展开的线性回归，其中25个中心均匀地分布在\[0,1\]间隔上。 右图：我们绘制拟合的平均值，对所有100个数据集求平均值。 顶行：强正规化：我们看到个体拟合彼此相似（低方差），但平均值远非事实（高偏差）。 底行：轻度正则化：我们看到个体拟合彼此非常不同（高方差），但平均值接近事实（低偏差）。 基于（Bishop 2006a）图3.5。 由biasVarModelComplexity3生成的图。

让我们假设我们先验的略有错误指定，因此我们使用$$\theta_0= 0$$，而事实是$$\theta^{\\*} = 1$$.在图6.4（a）中，我们看到对$$\kappa_0> 0$$的MAP估计的采样分布偏离事实，但比MLE有更小的方差（较窄）。

在图6.4（b）中，我们绘制了$${\rm mse}(\tilde{x})/{\rm mse}(x) \ {\rm v.s.} \ N$$ 。我们看到MAP的估计比MLE有更低的MSE，特别是对于小样本\($$\kappa_0 \in \{1,2\}$$\)。 $$\kappa_0= 0$$情况下对应于MLE，$$\kappa_0= 2$$情况对应于强先验，这会损害性能，因为先验均值是错误的。 “调整”先验的强度显然很重要，这是我们稍后讨论的一个主题。

### 6.4.4.2 示例：岭回归\(ridge regression\)

偏差方差权衡的另一个重要例子出现在岭回归中，我们将在7.5节中讨论。 简而言之，这对应于在高斯先验$$p(\boldsymbol{w})=\mathcal{N}(\boldsymbol{w} | 0,\lambda^{-1}\boldsymbol{I})$$下的线性回归的MAP估计。这个零均值先验鼓励小权重，这减少了过度拟合; 精度项$$\lambda$$控制该先验的强度。 使用$$\lambda= 0$$导致MLE; 使用$$\lambda> 0$$会导致有偏估计。 为了说明对方差的影响，请考虑一个简单的例子。 图6.5的左边绘制了每条拟合曲线，右边绘制了平均拟合曲线。 我们看到，随着我们增加正则化器的强度，方差减小，但偏差增加。

### 6.4.4.3 分类的偏差-方差权衡

如果我们使用0-1损失而不是平方误差，则上述分析会中断，因为频率派风险不再表示为平方偏差加方差。 事实上，可以证明（（Hastie et al.2009）的习题7.2）变成了偏差和方差相乘。 如果估计值位于决策边界的正确一侧，则偏差为负，减小方差将降低误分类率。 但如果估计是在决策边界的错误一侧，则偏差是正的，因此增加方差是值得的（Friedman 1997a）。 这个鲜为人知的事实说明偏差-方差权衡对于分类来说并不是非常有用。 最好关注预期损失（见下文），而不是直接关注偏差和方差。 我们可以使用交叉验证来估计预期损失，正如我们在6.5.3节中讨论的那样。

[**返回本章目录**]({{< relref "/docs/mlapp/06frequentist_statistics" >}})

