<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>线性回归 on 学习笔记</title>
    <link>https://chaoskey.gitee.io/notes/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
    <description>Recent content in 线性回归 on 学习笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>一切都是我的</copyright>
    <lastBuildDate>Tue, 23 Jul 2019 20:20:35 +0800</lastBuildDate><atom:link href="https://chaoskey.gitee.io/notes/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>7.6 贝叶斯线性回归</title>
      <link>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0065/</link>
      <pubDate>Tue, 23 Jul 2019 20:20:35 +0800</pubDate>
      
      <guid>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0065/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chaoskey.gitee.io/notes/notes/docs/mlapp/07linear_regression/&#34;&gt;&lt;strong&gt;返回本章目录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;虽然岭回归是计算点估计的有用方法，但有时我们想要计算关于
  
    

    &lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css&#34; integrity=&#34;sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq&#34; crossorigin=&#34;anonymous&#34;&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js&#34; integrity=&#34;sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js&#34; integrity=&#34;sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI&#34; crossorigin=&#34;anonymous&#34;
            onload=&#34;renderMathInElement(document.body,
                {delimiters: [{left: &#39;$$\n&#39;, right: &#39;\n$$&#39;, display: true}, {left: &#39;$$&#39;, right: &#39;$$&#39;, display: false}, 
                              {left: &#39;\\[&#39;, right: &#39;\\]&#39;, display: true}, {left: &#39;\\(&#39;, right: &#39;\\)&#39;, display: false}]});&#34;&gt;&lt;/script&gt;
    
  




&lt;span class=&#34;katex&#34;&gt;
  \(\boldsymbol{w}\)
&lt;/span&gt;
和


&lt;span class=&#34;katex&#34;&gt;
  \(\sigma^2\)
&lt;/span&gt;
的完全后验。 为简单起见，我们首先假设噪声方差


&lt;span class=&#34;katex&#34;&gt;
  \(\sigma^2\)
&lt;/span&gt;
是已知的，因此我们专注于计算


&lt;span class=&#34;katex&#34;&gt;
  \(p(\boldsymbol{w}| \mathcal{D},\sigma^2)\)
&lt;/span&gt;
。 然后在7.6.3节我们将考虑一般情况，也就是计算


&lt;span class=&#34;katex&#34;&gt;
  \(p(\boldsymbol{w},\sigma^2|\mathcal{D})\)
&lt;/span&gt;
。 我们假设始终是高斯似然模型。 以稳健拟然执行贝叶斯推断也是可能的，但需要更高级的技术（参见练习24.5）。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>7.5 岭回归</title>
      <link>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0064/</link>
      <pubDate>Mon, 22 Jul 2019 20:20:35 +0800</pubDate>
      
      <guid>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0064/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chaoskey.gitee.io/notes/notes/docs/mlapp/07linear_regression/&#34;&gt;&lt;strong&gt;返回本章目录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最大拟然估计(MLE)的一个问题是它可能导致过拟合。 在本节中，我们将讨论一种通过使用高斯先验的最大后验估计(MAP)的方法来改善此问题。 为简单起见，我们假设高斯似然，而不是稳定性拟然。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>7.4 稳健线性回归*</title>
      <link>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0063/</link>
      <pubDate>Sun, 21 Jul 2019 20:20:35 +0800</pubDate>
      
      <guid>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0063/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chaoskey.gitee.io/notes/notes/docs/mlapp/07linear_regression/&#34;&gt;&lt;strong&gt;返回本章目录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在回归模型中，使用零均值和常数方差的高斯分布对噪声进行建模是很常见的。
  
    

    &lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css&#34; integrity=&#34;sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq&#34; crossorigin=&#34;anonymous&#34;&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js&#34; integrity=&#34;sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js&#34; integrity=&#34;sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI&#34; crossorigin=&#34;anonymous&#34;
            onload=&#34;renderMathInElement(document.body,
                {delimiters: [{left: &#39;$$\n&#39;, right: &#39;\n$$&#39;, display: true}, {left: &#39;$$&#39;, right: &#39;$$&#39;, display: false}, 
                              {left: &#39;\\[&#39;, right: &#39;\\]&#39;, display: true}, {left: &#39;\\(&#39;, right: &#39;\\)&#39;, display: false}]});&#34;&gt;&lt;/script&gt;
    
  




&lt;span class=&#34;katex&#34;&gt;
  \(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)
&lt;/span&gt;
 ，其中


&lt;span class=&#34;katex&#34;&gt;
  \(\epsilon_i=y_i-\boldsymbol{w}^T \boldsymbol{x}_i\)
&lt;/span&gt;
。 在这种情况下，最大化拟然等价于最小化残差平方和。 但是，如果我们的数据中存在&lt;strong&gt;异常值&lt;/strong&gt;，则可能导致拟合不良，如图7.6（a）所示。 （异常值是图底部的点。）这是因为平方误差以二次方处理偏差，因此远离线的点对拟合的影响大于线附近的点。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>7.3 最大似然估计（最小二乘）</title>
      <link>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0062/</link>
      <pubDate>Sat, 20 Jul 2019 20:20:35 +0800</pubDate>
      
      <guid>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0062/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chaoskey.gitee.io/notes/notes/docs/mlapp/07linear_regression/&#34;&gt;&lt;strong&gt;返回本章目录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;估计统计模型参数的常用方法是计算MLE，其定义为&lt;/p&gt;

  
    

    &lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css&#34; integrity=&#34;sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq&#34; crossorigin=&#34;anonymous&#34;&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js&#34; integrity=&#34;sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js&#34; integrity=&#34;sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI&#34; crossorigin=&#34;anonymous&#34;
            onload=&#34;renderMathInElement(document.body,
                {delimiters: [{left: &#39;$$\n&#39;, right: &#39;\n$$&#39;, display: true}, {left: &#39;$$&#39;, right: &#39;$$&#39;, display: false}, 
                              {left: &#39;\\[&#39;, right: &#39;\\]&#39;, display: true}, {left: &#39;\\(&#39;, right: &#39;\\)&#39;, display: false}]});&#34;&gt;&lt;/script&gt;
    
  




&lt;span class=&#34;katex&#34;&gt;
  \[
\hat{\boldsymbol{\theta}} \overset{\Delta}{=} \underset{\boldsymbol{\theta}}{\rm argmax} \log p(\mathcal{D}|\boldsymbol{\theta})  \tag{7.4}
\]
&lt;/span&gt;</description>
    </item>
    
    <item>
      <title>7.2 模型选择</title>
      <link>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0061/</link>
      <pubDate>Fri, 19 Jul 2019 20:20:35 +0800</pubDate>
      
      <guid>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0061/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chaoskey.gitee.io/notes/notes/docs/mlapp/07linear_regression/&#34;&gt;&lt;strong&gt;返回本章目录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;正如我们在1.4.5节中讨论的那样，线性回归是一个形如下式的模型&lt;/p&gt;

  
    

    &lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css&#34; integrity=&#34;sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq&#34; crossorigin=&#34;anonymous&#34;&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.js&#34; integrity=&#34;sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;
    
    
    &lt;script defer src=&#34;https://cdn.bootcss.com/KaTeX/0.11.1/contrib/auto-render.min.js&#34; integrity=&#34;sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI&#34; crossorigin=&#34;anonymous&#34;
            onload=&#34;renderMathInElement(document.body,
                {delimiters: [{left: &#39;$$\n&#39;, right: &#39;\n$$&#39;, display: true}, {left: &#39;$$&#39;, right: &#39;$$&#39;, display: false}, 
                              {left: &#39;\\[&#39;, right: &#39;\\]&#39;, display: true}, {left: &#39;\\(&#39;, right: &#39;\\)&#39;, display: false}]});&#34;&gt;&lt;/script&gt;
    
  




&lt;span class=&#34;katex&#34;&gt;
  \[
p(y|\boldsymbol{x},\boldsymbol{\theta})=\mathcal{N}(y | \boldsymbol{w}^T\boldsymbol{x},\sigma^2)    \tag{7.1}
\]
&lt;/span&gt;</description>
    </item>
    
    <item>
      <title>7.1 导论</title>
      <link>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0060/</link>
      <pubDate>Thu, 18 Jul 2019 20:20:35 +0800</pubDate>
      
      <guid>https://chaoskey.gitee.io/notes/docs/mlapp/07linear_regression/0060/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chaoskey.gitee.io/notes/notes/docs/mlapp/07linear_regression/&#34;&gt;&lt;strong&gt;返回本章目录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;线性回归是统计学和（监督）机器学习的“驮马(work horse)”。 当对核或其他形式的基函数进行扩展时，它也能模拟非线性关系。 当用伯努利或广义伯努利分布代替高斯分布输出时，它可以用于分类，我们将在下面看到这点。 因此，详细研究这个模型是值得的。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
